
- dml 과 ddl 작업을 세션간에 배타적으로 처리하는 문제(ddl작업을 예약하고 일정시간 경과후 시작할때까지 모든 수행중인
   dml작업을 롤백하고 새로 시작하지 못하게) -> checkDmlJob에서 이미 구현됨
- 테이블 단위 제약조건을 각 컬럼에 표시하는 문제
- 트랜잭션 로그 pull중에 인터페이스 죽여 전환 시킴
- dll작업 이중화 
- dml 액티브 포워드 리턴 과 로컬실행 오버레이 구현

- DQI record attr val in/out
- SelMatrix record poiter out
- TEMP_VER도 temp key로 인증을하여 SYSDB를 처음 인증할때 생성하게 하고 
  이때 SYSDB에 종료날짜와 커런트 날짜를 저장하여 매일 커런트 날짜를 갱신하여 그 차이가 없어질때
  더이상 실해되지 못하게 한다. 만약 SYSDB를 삭제하고 기동하면 다시 인증 프로세스가 실행되고
  인증서버에서는 temp key는 각 기계의 맥 어드레스를 저장하여 이미 저장된 맥 어드레스로 다시
  인증 요청이 오면 인증을 거절하여 두번 이상은 새로 설치되지 못하게 한다.

메모리 디비 + 분산 파일 하둡 솔루션
  - 하둡 map-reduce함수를 sql의 서브쿼리로 통합, 
  - sql <-> 멥리듀스간 데이터 교환, 맵리듀스 서브쿼리 결과를 테이블로 변환, 테이블 동적 생성, 문법 디자인
  - 비정형 데이터를 맵리듀스로 처리, 하나의 메모노드 마스터와 다수 하둡 슬레이브 구조로 그룹핑하여
    메모노드 클러스터 버전으로 통합 -> 하둡을 이용하여 그룹 하나의 메모노드의 데이터 처리량 극대화
  - 하둡으로 인/아웃 하는 API를 sql 문법으로 통합

메모노드 디비에서 외부 블랍 타입을 하둡 분산파일로 저장, API설계

메모노드 파이프
  - 파이프 사용자 프로그램은 클러스터 각 메모노드에서 샤딩되어 실행 만약 실행 중간에 통합 플로우면
    실행 프로우상 이전 파이프 노드에서 통합 파이프 노드로 클러스터간에 크로스 전파

각 메모노드로 데이터 밸런싱하여 샤딩하여 노트노드를 할당하는 프로그램 필요.

노트 노드상의 자식 노드들은 관계디비에서 속성개념, 일반적인 그래프 디비 모델에서 속성을 
메모노드 그램프에서는 따로 지정하지 않음.

HDFS -> 메모노드 그라프(디렉토리에 해당, 메모노드 디비 클러스터 버전으로 무한노드 갯수 보장) + 각 노드에 단말 대용량 분산 파일 구현으로 대체 
맴리듀스 -> 메모노드 파이프 프로그램으로 대체 -> 비정형 대용량 데이터 처리에 사용 -> 메모노드 데이터베이스의 서브쿼리 결과로 통합
 
- 패시브 노드에서 ddl수행시 패시브 세션들의 워킹을 롤백시키는것 추가
- 스케쥴 획득과정에서 작업중인 세션 카운트할것이 아니라 qsrv 루프의 선두에 체크부분에서 하는것 검토
- 액티브가 종료할때 마지막 플러쉬를 전송하지 못하면 페시스에 마지막 트랜잭션 로그가 반영되지
  못하는 것 검토, 원래 트랜잭션 로그는 매번 커밋마다 전송되고 액티브에서 플러쉬를 못받더라도
  패시브에 저장된 트팬잭션 로그를 자체로 플러쉬할수있음 <- 액티브로 전환되는 시점에 액티브로부터
  받은 로그 플러쉬하는것 필요하지 않을까

- 이중화 패시브 노드 long commit상황에서 조회시 마지막 셀매트릭스 리턴전에 실렉트한 러닝 레코드중 
  자신이 dml한 워킹태그를 이용하여 워킹태그 리스트에 있은 러닝레코드를 제외시키는 것 추가 

- 패시브에서 이중화 트랜잭션 로그 푸쉬 반영중간에 인덱스가 맟지않아 reqPullTrsLog를 다시 수행하는데 이때
  데이터 결손이 발생하면
  기존 인덱스를 모두 삭제하고(releaseAllIndex()) 패시브 처음 기동시 prev overlap으로 수행되는 것과 마찬가지로
  수행하게 하는것 추가 검토

 external blob 추가
 utf8 encoding 문자열 함수
 파티션
 싱크
 넘버타입 사칙연산 에뮬
 외래키 체크 
k24so.cache buffer를 생성할때 블랍타입으로 하여 정수 정렬 시킨다.

funsion db type 구현 방안
- 프레임 인덱스에 등록되는 가상메모리 페이지를 실 페이지보다 크게하여 옵셋으로 실페이지를 접근하게하여 
  프레임 인덱스를 최대 4G갯수, 주소공간 포함 사이즈 32G, 이때 최대 가상 페이지 사이즈 4G로 한다.
- 디스크 기반 디비 타입은 테이블 심볼링크를 지원하여 메모리 기반 혹은 순수 메로리 기반 테이블을 링크하여 
  해당 테이블이 참조되면 이에 대응하는 메모리 기반 테이블이 엑세스되게 하여
  디스크 기반 테이블은 (btree인덱스, 조회시 레코드 복사) 작동하고 메모리 기반 테이블은 메모리 기반으로(avl index) 작동하게 한다.
- 레코드 락킹 메커니즘에서 레코드의 테이블이 디스크 기반이면 점유 레코드는 주소를 키로 복사된 레코드 포인터를 해쉬에 등록
- 디스크 기반 디비는 모든 로직에 메모리 기반을 포함한다.
- 메모리 기반 디비는 모든 데이터가 가상메모리 페이지에 로드되어 처리되고 디스크 기반 디비는 정해진 가상 메모리 용량 내에서 스와핑 처리된다.

- 싱크는 백업 디스크가 없으면 로컬 디스크를 대상으로 메모리 기반 저장타입 디비로 작동하고 
  백업 디스크가 있으면 백업디스크를 대상으로 디스크 기반 디비 타입으로 작동하며
- 백업 디스크를 부착상태에서 운영중일때
  메모리 기반 테이블은 백업 디스크를 제거할때나 일정 주기로 로컬 디스크의 내용을 페이지 단위로 백업 디스크로 다운로드 싱크시키고
  디스크 기반 테이블은 백업 디스크를 홈으로 작업하다 백업 디스크를 제거할때 최신 데이터를 페이지 단위로 로컬디스크로 업로드 싱크시킨다.
- 백업 디스크를 부착할때는 메모리 기반 테이블이나 디스크 기반 테이블 모두 로컬 디스크 변경된 페이지들만 백업디스크로 다운로드 싱크시킨다.
  
- 4가지 싱크 상태 존재
  1. 백업 디스크 미 부착 상태에서 운영 - 메모리 기반 테이블, 디스크 기반 테이블 모두 로컬 디스크를 대상으로 메모리 기반으로 동작.
  2. 백업 디스크 부착 - 메모리 기반 테이블이나 디스크 기반 테이블 모두 로컬 디스크 변경된 페이지들만 백업디스크로 다운로드 싱크,
			디스크 기반 테이블은 어드레스가 로컬 디스크나 백업 디스크나 동일하게 유지되고 내용이 일치됐으므로 운영중인 가상 메모리를
			그대로 운영하면서 저장소를 운영중에 백업디스크로 전환시키고 디스크 기반으로 동작(btree인덱스 갱신). -> 로컬 디스크와 
			백업 디스크가 사이즈가 틀린 상화을 고려하여 레코드 기반으로 싱크시켜야함
  3. 백업 디스크 부착 상태에서 운영 - 메모리 기반 테이블은 주기적으로 백업 디스크로 변경 페이지들을 다운로드 싱크, 
					디스크 기반 테이블은 2.번 상태에서 백업 디스크를 대상으로 디스크 기반 디비로 직접 운영되므로 싱크 필요없음.
  4. 백업 디스크 제거 - 메모리 기반 테이블은 할일 없고 디스크 기반 테이블은 최신 데이터를 페이지 단위로 로컬디스크로 업로드 싱크하고
			2번과 반대 상황으로 저장소를 백업 디스크에서 로컬 디스크로 전환시키고 메모리 기반으로 동작.
- 레코드 기반으로 싱크 방안 - 트랜잭션 로그가 업데이트는 삭제와 삽입으로 분해되므로 2번후에 첫번째 트랜잭션 로그에서 삭제/삽입 레코드 로그만 따로 압축로그로 저장한후
	두번째 트랜잭션 로그부터 현 트랜잭션 로그의 레코드 로그들을 하나씩 이전에 따로 저장한 레코드로그와 aTrsData로 주소 비교하여 2번 과정 이후에 삽입된 
	레코드(TRS_RECORD_NEW로 표시됨)가 삭제되는 경우는 압축 로그에서 제거하여 압축 로그를 매 트랜잭션 로그가 저장될때 마다 새로 구성한다.
	이후 백업디스크로 싱크할때 이렇게 압축된 로그의 레코드들을 sql로 익스포트하여 백업디스크에 반영한다.

- 이중화 트랜잭션 푸쉬할때 블랍테이블인 경우 페이지 할당 트랜잭션 타입 전송
- 이중화 반형 함수에서 풀 리두로그한 결과 푸쉬 사작 인덱스 보다 더 가져온 경우 다음 푸쉬 로그 인댁스를 풀 인덱스에 맟추는 것 추가.

- 싱크방안
  순순 메모리디비, 디스크기반 로컬 저장 < -- > 디스크 기반 외부 저장간에 변경 레코드만 싱크
  레코드및 페이지에 싱크 이후 삽입되는 레코드 및 페이지에 마킹, 마킹되지 않은 레코드 삭제시(업데이트는 삭제, 삽입으로 분해) 트랜잭셕 로그에서
  sql로 변환하여 따로 저장, 이후 싱크 시점에 전체 패이지 스캔하면서 마킹 레코드만을 삽입문으로 출력하여 앞서 보관한 삭제 sql과 합쳐 타겟 디비로 임포트
  스캔하면서 레코드 및 페이지 마킹 리셋하여 다음 싱크 과정에서 사용
  테이블 alias제공하여 통합디비 지원
  차이점 - 하나의 디비엔진에서 위 디비 타입을 해제, 로드해가며 운영, 양방향 싱크 가능, 테이블 alias에 의한 디비 통합

네트웍 속도 - 11초
파싱 속도 - 42초
실행 속도 - 24초
커밋 속도 - 2초

select 2 * 1024 * 1024 * 1024 from dual 
를 실행하면 -2147483648 라 나오고,

select 4 * 1024 * 1024 * 1024 from dual 
를 실행하면 0 으로 나옵니다.

수치의 범위를 넘어서기 때문에 발생한거 같은데,
요즘 화일의 사이즈 또는 대용량 건수를 가지고 연산하는 경우가
있어서 수정이 되어야 할 듯 싶습니다.

- 디스크기반 디비에서 대량데이터 일괄 dml 수행의 경우에 
	1. 레코드 락킹 설정을 하지않고 러닝디비에 바로 반영하면서 트랜잭션 태깅될때 데이터를 파일에 쓰며 그 포인트를 
		clone reflection과정에서 승계하여 저장하게 한다.
	2. long commit단계를 두어 dml시작시에 배타스케쥴 설정하고 dml조회에서 전체조회 건수가 일정이상을 넘어가면 이한 잘라버리고
		커밋단계까지 수행후 리두로그 저장에서 시작 시점을 기록하고 배타 스케쥴을 반납하지 않는 상태에서 dml조회부터 다시 실행하에하여
		리두로그까지 dml조회가 더이상 건수한계에 걸려 잘리지 않을때까지 계속 반복실행하게하여 끝낱을때 커밋 시퀀스 증가하고 저장디비에
		반영하도록 한다. 중간에 에러가 발생하면 앞서 저장한 시작 리두로그 시퀀스로 복원케 한다. -> 롤백 로그로 롤백한다.
  1이나 2방안 둘중에 하나 2가 유력

  조인튜플 리스트에서 조인 누락된 페이지 찾는 방법 - 조인튜플 구성 순서대로 조인페이지 리스트 구성하여 이번에 조인된 튜플의 페이지가 조인페이지 리스트의
	헤드 페이지가 안니면 헤드메이지는 누락된것 그 다음 페이지도 같은 방버으로 반복

- workSign에 조회 레코드를 복사한 레코드라는 표시를 하고 레코드 시작 포인터 앞에 레코드 주소를 설정한다.
- touch load할때 디스크 로드를 별도 쓰레드로 오버랩하여 처리하게, 로드한후 요청 쓰레드에 시그널
- 이중화 패시브에서 커밋명령을 롤백으로 변경

- 순환 재실행시 스키마 변경 체크(순환재실행 시작시점에 워킹디비, 테이블 등록하여 이후 재실행 스케쥴 획득할때 스키마 변경체크되게)
- 주기적으로 해제 페이지 수거 (순환재실행 시작시점에 대상 테이블에 순환실행 카운트, 완료시에 디카운트하여 해제페이지 반납에서 카운트가 있으면 반납 수행안되게)
- 페이지 트랜잭션 태깅은 이중화 전송되도 패시브에서 실행되지 않으므로 이중화 고려하지 않고 페이지 해제/반납은 각 노드에서 독룁적으로 수행

- reent imem매번 릴리즈 -> 리셋 ( 할당시 뮤택 블럭 오버헤드를 피하기위해 )
- 삽입과 순환 쿼리를 동시에 하면 페이지가 증가되어 분할구간이 변경되면 어떻게 되나.
- imemBwdCycle도 mdev를 할당하여 할당, job->vmLockMade로 뮤택블럭 하지 말고
- tms agent 예외처리
- load db 매번 패스워트 체크 개선

- session의 cntPropagate 리셋 시점에 리셋
- 테이블 나눠 전파
- 코디네이터에서 2차 전파된 것 실행.
- wdbPropagate 해제
- 로컬 조인이 먼저 수행되고 완료후 분산 전파가 되면 문제가 현 릴레이션 전파 결과를 다 모두 취합 사용전에 또 로컬전파가(순환재실행이므로)
	되는 문제가 있어 분산 전파 로컬조인후 백점프 실행때 분산취합되면 순환실행 스택을 스위치하여 취합부터 먼저한다. 이때 토글도 같이 스위치해야함.
	-> smem, qmem같으 것들이 순차 할당되므로 스위치할수없음. 첫번째는 로컬조인을 먼저한다해도 두번째 부터는 분산전파조인 취합이 모두 끝나야 
	남은 로컬조인을 할수있으므로 어차피 수행속도는 같다. 따라서 무조건 분산조인 전파 취합 완료후 로컬 조인을 수행하게 한다.
- ClusSessionExec의 ClusRelatePropagation호출부분 제거
- 코디네이터에서 세션 깊이 설정 문제
- 백터 팩킹시 소스도 팩킹
- 백터 분할 전송
- 푸쉬된 백터 순환재실행 structured에의해 없어짐.

분산 버전 이중화 방안. - depricate
- 브로드 케스트를 두 그룹으로 나누고 현재 하나로 되있는 cnet리스너를 브로드케스트 리스너와 유니케스트 리스너로 나눠서
	브로드케스트는 각 그룹별로 전파되고 유니케스트는 각 그룹을 크로스오버하여 케스트 할수있게 브로드케스트 네트웍 카드와
	유니케스트 카드를 구성하여 각 리스너에 바인딩한다.
- 브로드케스트 그룹내의 각 노드별로 상대 브로드케스트 그룹의 노드와 이중화 구성을 이중화 카드로 따로 설정한다.
- 각 브로드케스트 그룹별로 클라이언트로부터 요청을 접수받은 노드가 코디네이터가 되어 코디네이터 노드만 sql요청을 이중화 노드로 포워드하면
	요청은 상대 그룹으로 넘어가서 두개 그룹에서 전파된다.
- 조회및 dml sql요청은 코디네이터에 의해 두 그룹에서 전파되어 실행되고 
	dml 요청은 두 그룹의 각 노드별 이중화 구성으로 마스터 노드에서 수정되어 슬레이브에 트랜잭션 로그로 반영되고 유니케스트로 마스터 노드에서만 
	사이클 체커에 제약조건 체크및 레코드 락을 질의된다.
	조회 요청은 두 브로드케스트 그룹의 노드들에서 마스터 노드들만 

분산 버전 이중화 방안. 
- 브로드캐스트 및 유니케스트 분산 네트워크를 전체 하나로 구성하고 그 안에서 각 노드들을 pair로 이중화 네드퉉 구성한다.
	즉, 분산 리스너와 이중화 리스너를 각기 별도의 카드로 로드한다.(리스너는 별도로 로드하고 카드는 하나로 해도 됨)
- 클라이언트로부터 요청을 접수받은 노드가 이중화 상대노드로 포워드하는 것을 제거하고 클라이언트로부터 요청을 접수받은 코디네이터 노드가 
	dml sql이면 sql을 브로드케스트하고 각 노드는 이중화 마스터 노드들만 sql을 실행하고 실행결과를 이중화 슬레이브 노드에 트랜잭션 로그로 반영한다.
	조회 sql이면 코디네이터가 단독으로 수행하며 릴레이션단계에서 분산 릴레이션을 브로드케스트하고 이중화 마스터 노드들만 분산 릴레이션을 실행하여
	그 결과를 코디네이터가 취합한다.
- 이중화 슬레이브 노드도 코디네이터가 될수있고 이때 슬레이브 노드는 dml은 성공여부만 취합하면 되고 조회는 로컬 조인은 수행하지 않고 분산 릴레이션 결과
	만을 취합하여 클라이언트에 응답한다.(액티브 노드로 포워드 하는것 제거)

a1		a2
b1		b2

a1-b1, a1-b2, a2-b1, a2-b2
a1-b1, a1-b2, a1-b2, a2-b2

죽은 소스 실시간 체크 방안
- 브로드케스트후 무조건 ack을 받아 카운트하여 전체 소스갯수 파악후 wait return에서는 max시간으로 대기하고 return sorce에서는 1초 이내 그 갯수만큼 만족될때 시그널
수신측 스템프 증가는 broad end에서 증가

dqirel에서 순환실행되면 jmat를 rp에 보관하고 이후 getjmatrix하지 않고 이를 재사용, ClusReentRelation에서 not overwrite라도 jmat 복원

call procecution이 실행되지 않으면 CombineProcJmatRes REENT_OUTJ _proc_bind->commonProc 가 설정되지 않아 에러

rear session 레지스터 슬롯 front세션의 것을 복사할때 전체 복사해도 되는가 문제, result state만 복사하는 것이 아니라
rear session 리셋 시점, 임의 세션을 최초 생성한 노드가 아닌 다른 노드에서 코디가 될 경우 rear세션도 원래 rear session으로 재설정 문제
no result send시 데이터가 없어 end packet이 덮어써지는 문제
begin casting에서 스템프 증가하면 릴레이션 시퀀스에서 후속 릴레이션 전송하면 스템프 증가되 스템프 불일치 발생

분산순환 종료위해 카운트 최대값 8바이트 정수로
dual전파 안하게

sqlbd에서 기본 카운트 2로 설정한것을 늘리게, 조건절 서브쿼리도 순환그룹함수에서 한번에 출력 ?

테이블 구초제 포인터 갯수 만큼 패딩한것 일치시킬것, qqqqqq 제거, 2차 전파 동기화 살릴것
unicastmessage2, broadcastmessage2 (ack바로 받고 waitSorceReturn으로 리턴받음) - alloc session, alloc sorce를 사용
두개 다 한개 소스에서 리턴 받을 때 dead sorce 체크
endCasting 개 소스에서 리턴 받을 때 dead sorce 체크 개선 -> outval이 있으면 한개 바로 리턴

미처 실행 완료되지 못한 소스 요청 제거
캐스터에서 수신완료 마킹 체크할때 소스 갯수로 먼저 체크
재접속 소스 아이피로 체크하여 소스아이디 재송

인지
어느 정도 큰 윈도우 사이즈 커널 학습 => 부분패턴(자주 반복되는)을 커널로 정의 => 어셈블리 임의 사이즈로 정하는 것은 드롭 => 자연스럽게 어셈블리 단위 생성, 
생성된 어셈블리 커널 패턴 타입으로 2차 학습( 이것도 어느정도 사이즈로)

1차 생서오딘 커널 패턴 타입으로 스라이딩 했을때 위치별로 나타나는 커널 패턴 타입으로 순서 생성 => 2차 학습은 순서 이미 학습, 1차 학습에서 그 결과로 커널 타입을 재비치
했기 때문에 2차때부터는 순서 둬도됩
커컬 사이즈의 반씩 겹치게 스트라이드하여 윈도우 사이즈만큼의 부분영역 학습 => 1차 인지망 => 학습 결과로 1차 데이터 평가, 똑같이 스트라이딩하여 나온 부분 패턴들을
커널 윈도우 위치에 표시

연속 데이터 자동 등급(커널과 같은 모든 연속 데이터에 사용)
- 적분 가중치 강도 범위 설정, 선 가중치 강도의 범위에 들어오는 후 가중치는 선 가중치의 대표로 라벨링 하여 패턴 학습
- 빈도수가 약한 대표 라벨 가중치 강도는 망각되고 우성 가중치 강도만 남게됨.
- 대포 가주잋 강도가 반복되며 가중치 강도의 적분 평균을 산출, 학습 패스가 진행되면서 적분 평균의누적이 발생하고 이것은 평균이 중첩되는 것이고 추록의 정확도 향상
- 평가 할때는 추론 패턴의 시작부터 가중치 푱균을 적분하여 추론 겨로가 수치 산출, 평가할때 percep패턴의 실재 값들은 추론을 하기위한 값이 아니므로 가중치 평균에 의하여
 적분되는 값이 추론 값이됨

연속데이터는 최소값과 최대값으로 nomalize하여 사용
데이타의 항목이 여러개이면 이것을 각 디멘젼 차수로함.
이멘젼 데이터를 시퀀스로 위치시켜 디맨젼 데이터간에 크로스하여 패턴 학습되게 한다.
연속데이터의 자리수별로 0 - 9까지의 숫자를 시퀀스로 위치시켜 패턴 학습되게 한다. 이렇게 하여 연속 데이터도 패턴 학습할수있다. 이때 라벨링은 시퀀스 열 구분 + 자리수 구분 + 0-9까지의 데이터

번역(예측) 가중치가 percep의 여러 경로를 통하여 도달하면 각 경로(인식패턴 연결)의 가중치 적분값들의 평균(강도,빈도가 곱해진)을 구하여 최종 에측값을 산출한다.

이미지 컨볼루션
- 적당히 큰 사이즈 커널 윈도우를 슬라이딩하여 커널내 부분 이미지를 패턴 학습, 평가 할때 동일한 방법으로 슬라이딩하여 학습된 패턴을 발체하여 패턴 아잊를 해당위치 커널 윈도우에 둡, 이때
 윈도우 사이즈를 벗어나 중간에 걸리는 패턴을 제외, 윈도우를 반씩 중첩되게 스트라이드하므로 의미있는 패턴을 다 걸려짐. (자동등급에의해 변화된 지점의 값으로 패턴화 되므로 위치 영향 없음)
 - 2차 인지망의 2차 학습에서는 커널 사이즈를 더 크게하여 1차 평가된 패턴 아이디 자체를 이미지 데이터로 하여 중첩 스트라이드 스라이딩하여 2차 학습 이렇게 반복하여 커널 사이즈가 
  전체 이미지 사이즈가 될때가지 진행, 최종 인식된 상위 패턴아이디가 분류되는 이미지 타겟이 된다.
 - 2차 학습에도 윈도우 슬라이딩을 하는 이유는 패턴의 상호간에 공간적인 상대 위치를 이용하여 어셉블리에 이용되게 하기위해
 - 2차 학습에서 패턴의 원 이미지 상대 위치는 커널의 위치로부터 상대 위치를 패턴의 아이디 시퀀를 옵셋으로 더하면된다.

 적분 가중치 강도 구하기
 적분값 A, 가중치 강도 W, linear value L
 A * W = L , 기존 적분값 A에 얼마 가중치 강도 W를 곳ㅂ하면 다음 linear값 L이 되는가
 W = L  / A

 W값을 구하여 임의로 정한 W값 범위(동일한 것으로 labeling하는)내이 겂들을 W값 푱균을 구하여 동릴 라벨링
이때 시게열 데이터는 동일 라벨 데이터도 각 시점별로 시퀀스 데이터로하여 패턴화 하고 이미지는 동일 라벨은 W값 평균에만 반영하고 시퀀스에 추가하지 않아 토폴로지 스케일 등에 
무관하게 한다. 즉 사컨만 기론되는 방식( 위치가 아니라)

이미지 Wk값 라벨을 윈도우를 슬라이딩(중첩되지 않게) 하여 해당 윈도우 슬라이딩 위치에두고 윈도우 사이즈는 두배씩 확대하여 이전 차수 인지망에서 인식한 패턴을 데이터 로서 2차 인식한다.
=> 컨셉 뷰잉, 공간적인 패치 정보 활용

이미지는 RGB를 디메젼으로하여 각 윈도우 별로 윈도우내에 시퀀스 데아타 로서 처리하여 함깨 패턴화 될수 있게 한다.

이미지에서 각 RGB값은 일정 농도 이하는 무시한다.

디비 수정 (추가)
캐쉬인덱스 방식으로 페이지 단위 일정조건일때 주기적 저장, 트랜잭션 없고 커밋없는 디비 타입 추가

로테이트 데이터 학습은 로케이트 고려하여 학습시키는 것이 아니라 평가할대 조급씩 회전하여 점점 나아지게

시퀀스에 연속 불연속 정보를넣어서 패턴으 구성, 이정보에 의해 두개 패턴 결괍, 패턴간의 선후는 상위 2차 레벨 패턴의 시퀀스러 파악

빈도수적은 패턴 삭제는 전체 평균이 아니라 해당 패턴과 연결되는 패턴간의 국소적인 빈도수 비교로 적은 빈도수 연결을 삭제

생성모델은 하나의 이미지는 뷰를 틀리게 하는 방식으로 다른 데이터들을 생성하여 자가학습, 커널 윈도웅 스트라이드 틀리게 하여

패턴 생성시 보상에 따론 피드백 처리 추가 패턴을 역으로 가며 가중치 값소, 생성모델 학습에서 패턴 생성결과 부정 되먹임이면 음수카운트하여 다음 생성에서 양수가 될때 발현하도록 하고
이전 음수 최고치를 기억하여 양수가 될때 생성하여 발현 결과 또 부정 되먹임이면 그 최고치를 바로 설정하여 다음 발현에는 상당한 지연이 되도록하여 학습 속도를 빠르게 한다.

elim step percep - 조건 문제 progressPercep을 하지 않으면 앞으로 진행되지 않음 퍼셉 포인터가
sql문장에서 아이디 마크 함수 호출하면 퍼셉테이블 스케쥴 증가 문제
마인드 테이블 따로 이름 주어 생성 함수 명령 코드 별도 생성.

문자형이 있으면 무조건 이중 리스트로 입력
그외는 배열, 시퀀스 사이즈와 배치 사이즈 설정, 리스트는 배치 사이즈만
c 리스트 입력 api는 널 종료 문자열 블랍
두 경우 모두 페이지 단위로 전송
c api는 위 각각 경우로 배열 블랍 쓰기(배치 단위로 입력을 받아서 페이지내에 단위로 시퀀스 단위로 잘라서 전송하는 래퍼 api, 반대로 시퀀스 단위 입력을 모아 페이지 단위로 전송) 와 각 원소 쓰기로 준비

쓰레드홀드 강도, 인텐스, 일라스트 강도 모두 고정으로 하고 이미 한번 쓰레쓰 홀드 강도를 통과한 퍼셉은 강도가 넘었으느모 이후에는 계속 통과하게 한다.
이러면 성능이 문제가 되므로 퍼셉이 생성될때 익스클루, 인클루 강도 및 cntStren, 등 변경되는 멤버들을 구초제로 정의하여 퍼셉 생성시 퍼셉 저장 레코드에 그 구조체 포인터를 기록하여
이 구조체를 퍼셉 레코드 조회하여 참조하고 메모리에서 강도변경하고 학습이 끝날때 일괄 레코드 저장한다. 구조체 생성할때 리스트를 구성한다.

peterming cut에서 inference best에서 전역으로 리셉 최고 길이 인클루 강도등을 오픈한 정보를 이용하여 컷한다.
마지막 리셉 stv를 전체 시퀀스 요소롤 산출하는 것.
인터브 컷의 강도를 늘려 퍼셉 생성 거부율을 줄이는 것.
스트라이드를 촘촘히 하여 학습.
기울기 피겨값 올리는 것
데이터를 업 다운으로 생성 학습.
데이터를 하나씩 증감으로 생성 학습.
다수결.
지니계수의 B값으로 계속 산출값으로 하여 입력 최종값과 차이가 가장 적은 것으로 선택.
입력 시퀀스 엘레먼트값의 합으로 목표값 산출, 목표값을 반씩나우어(남은 것의 반을 또 나누고, 100이면 50, 75(50 + 25), 89(50 + 25 + 14) ,, 와같은 식으로) 
각 시퀀스에서 50, 25, 14가 지니계수의 A값이 되게 계수 산출, 갈수록 목표값에 근접, 목표값을 한개로 ..

시퀀스내 어쩐 스팟과도 반복적인 연결이 없는 스팟은 잡음이다.
빈도수가 적은 기울기(스팟)가 컴볼류션 실행에서 탈락될일 없다. 하위에서 빈도수 높은 기울기 시퀀스가 상위로 가면 하나의 스팟이되고 이것과 빈도수 적은 스팟이 기울기로 연결되므로 
빈도수 적은 깅루기(스팟)도 포함된다.

convLength내에서 반복되는 에레먼트는(억셉터, 퍼셉) 스킵하여 한개인 것으로 취긊하여 스케일 차이에도 동일한 것으로 판단되게 한다. 상위로 올라가면서 이 과정을 반복하여
최상위까지 컨볼루션을 진행하고 최상위에서 목표값을 매칭한다. 검색과정에서 최상위에서 매칭되는 목표값이 없으면 그 하위 컨브로 내려가 검색하여 스케일 상의 다양한 차이에 가장 
적합한 스케일 매칭이 이루어 지도록 한다. 따라서 하위레벨 컨브에 스캐일이 배제된 주요 특징만의(공통 형태) 목표값 연결이 이루어지고 상위로 갈수록 스케일이 커지는 객제의 
목표값 연결이 이루어 진다. 이미지에서 첫번째 학습은 일정 영역(커널 윈도우)사이즈로 전체 배치 이미지를 스캔하여 작은 local에서 부호화를 하고 이 부호화된 코드 컨셉들로
그 이상 상위를 채널 구성없이 컨브 학습한다.

인공지능은 combination에 관한 것이다. 하위에서 부터 엘레먼트 쵀대 중복을 허용하여 강도순으로 시퀀스를 뽑되 입력 시퀀스의 엘레먼트가 모두 소진될때까지 검색 시퀀스를
뽑는다. 최대 중복 조합을 허용하는 것은 3가지를 유도한다. 첫째는 어디에도 조합되지 않는 엘레먼트는 이 컨브레렙에서 타겟 조합을 수행해도 당연히 타겟 조합에 포합될 수 없다.
둘째는 이 엘레먼트를 상위로 올려도 상위에서도 어디에도 조합되지 않는다. 셋째는 하위에서 완전 조합(반대는 부분조합)이 되면 상위도 완전조합이 된다.
따라서 2개 시퀀스가 남을때가지 컨브 연산하여 목표값과 조합한다. 이유는 예를들어 3개 시퀀스 a, b, c가 있고 완전 조합된다 할때 이 완전조합 abc과 복표값 조합 하나만 특징이 생성되는데
다시한번 상위 컨브하면 (ab)c, (ac)b, a(bc) 3개 특징 패턴 조합이 발생하여 이와 목표값 조합하면 3개의 표준값이 생성되므로 좀더 특징을 잘 조합할수있다.
결국 얼마나 입력 시퀀스로부터 생성되는 특징 조합이 상세하고 특징을 잘 반영하여 생성되고 이것과 목표 표준값이 바인딩되는냐가 학습이다.

최대중복 조함은 조합 엘레먼트들이 이전에 선택된 조합 엘레먼트들과 모두 겹치지만 않으면 허용하는 것이다.

신경망 오류 역전파에서 목표값을 입력 패턴에 반영하는 것과 같은 조합학습의 것은 동맹(조합)이다. 특정 목표값이 있을때의 입력 패턴 시퀀스는 컨브에서 단계적인 조합(동맹)으로 특징될수있고
상위 컨브로 올라가면서 하위의 고장도 조합 시퀀스는 상위의 한개 스팟(조합)으로 대변되며 상위에서는 강도 카운트가 하위에서의 승계없이 독립적으로 카운팅되므로 약강도 퍼셉 시퀀스 조합과 동일한 조건으로
조합되므로 하위에서 올라온 약강도 조합을 배제하고는 conv length를 만족하는 조합이 이루어지지 않으므로 약강도 조합이 배제되는 일은 없다.
conv length가 적을 수록 약강도 조합이 배제되지 않고 세밀한 특징을 추출하게 된다.

중복강도에 따라 조합을 선 추출할때 이전 선택된 조합과 엘레먼트 강도가 덜 반복되는 것으로 한다. -> 이럴 필요없다. 위에서 어느것돠도 조합되지 않는 조합시퀀스는 다른것과 완전 배타적인 조합이며
이 조합이 발생했다면 해당 컨브레벨에서 다른 조합은 나타나지 않을 것이고 deterministic하므로 이조합과 바로 목표치를 연결 조합하면 된다. 따라서 약강도 조합이 완전 패타적이여서 
상위 컨브로 올라가지 못하고 배제되는 것을 걱정안해도 된다. 이 상태가 바로 결정적 상태이므로 학습된것이다. -> depricate

초대 중복 조합에 따라 약강도 퍼셉 시퀀스가 배제되는 것은 없다. 이유는 학습단계에서 한번의 입력 시퀀스에만 등장하는 약강도 퍼셉일지라도 최대 중복 허용을 하고 한번의 퍼셉시퀀스도 상위 컨브로
올려 상위 조합을 하게한다면 학습단계 조합에 약강도 퍼셉이 있는한 상위까지 조합되어 중간에 배제되지 안는다. 배제되는 경우는 예를들어 입력 시퀀스가 일부분이 완전 분리되어 학습단계에서의 
조합에 없는 조합이 테스트 단계에 올때인데 이 경우도 모든 조합을 선택하는 기준이 최고(최선)강도로 선택되므로 강도가 높다는 것은 반도수가 많다는 것이고 이것은 여러 경우에서 공통으로 등장한다는
것이므로 약강도 퍼셉시퀀스와도 연결이 없을수 없으므로 이 경우도 약강도 퍼셉과의 조합은 반드시 있다. 따라서 배제되는 경우는 완전히 분리되는 경우인떼 이러한 케이스는 학습이 될수 없는 경우이다.
즉 학습단계에서는 양립된적 없는 두개가 평가 단계에서 발생하는 경우이고 이럴때는 둘줄 저강도를 버리고 더 고강도를 상위 컨브로 올린다.

최하위 차 상위 컨브 레벱부터 목표값 조합을 수행하고 평가때 배제 break가 발생하면 그 하위 레벨 컨브에서 목표값 예측을 수행한다.

고양이 공통 사자가 있고 고양이와 사자는 서로 배타적일때 공통을 포함한 배제의 문제는 공통을 포함한 어느쪽을 선택하는가 문제, 강도가 높은쪽, 2개 시퀀스가 남을때가지 컨브 연산
하위는 고강도 스팟으로 쏠리는 것을 막기위해 partial conv, 상위는 최적의 조합을 위해 conceptual conv를 수행하고 강도 선택은 seq length * 단위 + exclusive 강도를 기준으로
하위는 최고강도 대표값 비교인 pax로 추출하고 상위 컨브는 모든 것 선택에의한 차선 선택.
고양이 데이터는 고양이에 맞는 패턴이 강도가 높게 나오고 사자 데이터는 사자 패턴이 높게 나올거라는 믿음. 배제는 deterministic한 결정의 과정.
현재는 모든것을 선택하고 나중에는 pax를 적용.
하위에서 conceptual conv를 수행하면 하나의 고강도 스팟과 이 이전의 모든 스팟이 조합되어 결국 앞선 스팟들을 나열해 놓는 결과만 초래하여 다향한 조합이 되지 않을 것.
상위 컨브에서 3개 스팟이 남고 그증 하나가 완전 배제되면 이 컨브단계에서 목표값과 조합하고 모두 조합이 있으면 2개 조합을 생성하여 상위 컨브로 올려 목표값과 조합.

이미지 스케일 극복 방안 - 스케일에 상관없을 정도의 작은 단위 컨브길이로 최 하위 partial conv하여 부호화 조합생성, 그 상위부터는 조합퍼셉이므로 위치가 빠짐, 
컨브 길이를 고정하지않고 이미지 사이즈의 크기에 비례하여 설정, 즉 예를들어 일률적으로 이미지의 6등분한다면 이미지가 크면 비례하여 컨브길이가 커지고 학습때 생성되는 패턴조합은 
평가때 다른 사이즈의 이지미에서도 검색될수있다. 이 상위 conceptual conv에서는 컨브길이를 고정해도 됨.

이차원 이미지는 행과 열의 각 셀을 직조하여 썩어서 1차원 데이터로 만든다. 직조란 1행의 셀 사이 사이에 2행의 셀을 삽입하고 3행은 2개 사이, 4행은 3개 사이 이런식으로 썩는 것.

컨브 결과를 목푝값 조합을 생성 학습할때 입력값은 퍼셉아이디이므로 입력 시퀀스는 0로 설정하여 입력은 스키마은 업는 것으로 encode symbol에서 입력값을 읽고
입력시퀀스가 결정될때 그 시퀀스 갯수를 베이슥값으로 cleo에 설정하여 목표값 acceptith는 이 베이슥값을 빼서 순수 목표값만의 인덱스로 만들어 make percep key에서 이 acceptith는으로 
목표값 퍼셉의 키를 생성하고 저장한다. 평가시에도 마찬가지로 최종 컨브의 입력 시퀀스가 결정되면 그 시퀀스 갯수를 베이스로 설정하여 목표 퍼셉이 선택되게 한다.

최하위 컨브 부호화 학습때 최소 한개 채널의 배수로 컨브길이를 잡는다. 채널 스키마는 컨브 연산를 의식하지 않고 구성한다.

목표값 연결 조건을 레벌이 아니라 장면별로 시퀀스 갯수로 한다. 갯수가 일정 수 이하로 남았다는것은 그 레벨이서 그 입력 시퀀스들이 deterministic해졌다는 것이므로 이러한 것들만
목표값 연결 학습하면 평가할때도 낮은 레벨에서도 deterministic한 결과가 나올수있다.

번역 시스템
컨볼루션 하위부터 각 레벨별로 evaluate단계에서 에레먼트가 연속하지 않고 떨어진 패턴은 그 불연속 부분에 anonymous태그 아이디를 삽입하여 상위로 올림.
상위에서는 익명이 포함된 에렐먼트 시퀀스가 학습되고 최초로 익명이 학습되는 이 레렘에서 원문과 번역문간에 사이즈를 2로 하여 컨브(연속한 패턴만 학습)학습과 원문 번역문 일대일 연결을 학습한다.
후에 상수나 고유명사와 같은 것들을 위 연속한 패턴과 원문과 번역문간의 위 레베렝서 일대일 매칭된 패턴을 이용하여 적절한 위치에 익명 교체 처리한다. 
예) A(익명) <원문 -- 번역문> B(익명) 일때 원문의 익명에 해당하년 상수를 번역문 익명에 대체, 또한 익명을 메타 코드와 소통하는 매카니즘으로 사용한다.
예 1 시 에약 할수있나요 ? -> "( 1 ) 시 예약" 이라는 패턴 생성되어 메타 코드에 질의하면 메터 코드에서 호스트 reflection code호출하여 "( 1 ) 시 예약" 이라는 정보를 주고
호스트에서 "( 2 )시 예약" 이라는 답을 주면 미리 하습된 2 시 예약 가능합니다. 라는 등답에서 익명 부분 1 -> 2 로 교체하여 응답한다.
원문과 번역문을 각각 위와같이 학습후 최상위 regress결과로 원문과 목표값 매칭을 fully connected로 연결 학습(빈도수가 낮은 위드들은 익명 처리됐기때문에 최상위 regress결과 매칭은
일대일 완전 매칭될수잇다.)

조합 단계에서 번역문을 하위로 전개하여 하위에서 익명부분에 위 학습 결과로 원문의 익명 처리된 워드에 적함한 번역문 워드를 번역문 익명 부분에 삽입한다.
절도 상위 컨브에서 익명으로 처리한다.
프로그램잉 언어 번역은 익명으로 처리되는 부분이 숫자일 경우 한계값, 첨자값 등과같은 품사 태그로 미리 변환 전처리 하여 학습한다.

응답 시스템에서 리플렉션 코드는 응답 문장 시퀀스의 최종 아이디가 응답 문장의 의미를 대표하여 이에 맞는 코드를 작성할 수 있고 익명 태그 아이디(익명 옆에 존재하는 패턴아이디)를 키로하고
익명 값을 밸류로 하는 키/밸류 쌍으로서 상황에 맞는 밸류 값을 대체하여 문장 패턴 아이디, 익명 태그 패턴 아이디, 익명 대체 값을 리턴하면 메타 코드에서 조합된 응답 문장을 전송한다.

테스트 마이닝
문장 단위로 테스트 마이닝 학습하고 이와 목표값을 연결학습하여 목표값과 일정부분 연관성이 있는 패턴들만 예측을 위한 학습 시쿼스에 포함시켜 학습.

양측 구조 연결 번역에서 구조란 빈도수 내림차순으로 오더링된 것이므로 레벨별 연결 학습은 양측 빈도수 단위로 매칭하믄 것이므로 의미 있다. 언어는 복문 학습에서 한번만 중첩되는 복문까지막 학습한다.

elim stem percep 제거 검토, 표준편차로 기울기 계산

자율학습 - 퍼셉션 네트워크 간의 연결학습이던 퍼셉션 네크워크 자체 학습이던 그 입력 시퀀스를 먼저 실행해 보고 긍정적 결과이면 입력 시퀀스를 퍼셉션 간에 연결 학습 또는 퍼셉션 학습을 시킨다.

정의학습(비지도학습) - 학습때 최상위 출력 결과를 하위 각 레벨별로 연결학습, 후론때 최종 레벨 결과(학습때 레벨에 미치치 못해도)로 베이즈 확률 추론하여 학습때의 최종 결과 예측
연결학습(지도학습) - 입력값, 목표값 각각 정의학습후에 문장별 최종 레벨 연결학습하여 베이즈 확률 추론, 타겟 예측(산출)하여 입력값 하위 레벨 연결학습, 학 레벨에서 패턴별로 타게팅(베이즈 추론)하여 연결학습 
강화학습(자율학습)
	퍼셉션 정의 학습 - 서킷(쓰레드) 한개 혹은 다수개로 각 레벨별로 입력값을 패터마이닝 행렬 전개 방법대로 시퀀스 발생시켜 실행해한후 보상 매커니즘에 따라 결과가 긍정이면 그 시퀀스를 입력시켜
			패터마이닝 수행하여 패턴 생성, 저장
	퍼셉션간 연결학습 - 각 퍼셉션을 장면별로 실행하여 긍정 결과이면 연결 퍼셉션 네트워크간 연결학습, 저장, 결과가 부정이면 연결학습 수행하지 않음.
번격 - 최상위 피쳐 및 각 레벨별 엔딩 피쳐을 입력과 목표간에 연결학습(하위까지 진행)하여 카운터 파트 확립, 혹시 양측간에 최상위가 틏릴 경우 최상위와 엔딩 피쳐를 양측 같이 연결 학습
	- 구조 학습때 각 레렐에서 빠지는 엔딩 피쳐 위치에 유나이티드 패턴 삽입하여 상위로 올림, 안롤릴 때아 마찬가지로 상위에서는 유나이티드 양측이 연관이 강하면 하나의 패턴으로 바인드 될것임.
	- 평가때 원문의 각 레벨별 엔딩 피쳐의 빠진 위치는 유나이티드 패턴 위치에 언딩피쳐를 차례로 대응시키면되고 번역문도 똑같이 원문과 대응되는 빠지지않는 패턴 시퀀스의 유나이티드 위치에 원문과 대응되는 엔딩피쳐를 삽입
	- 빠지지 않는 피쳐는 어순을 지지하고 엔딩피쳐는 의미상 원문과 대응(같으므로)하므로 원문과 같은 유나이티드 위치에 넣으면 된다.
	- 생성모델이라는 것은 원문, 번역분이 바로 일대일 대응이 아니라 하나의 중간단계(중간단계가 여러 단게로 될수있믐, 또는 추상적인 단계, 심층단계, 심츧단계의 변이)를 거쳐 최종 번역됨을 으미한다.

normalize(universal code) - 시간스템프 별로 모든 입력이 크로스되어 각 담당 처리별로 차원축소 인코딩하여 이것을 seed라하고 모든 처리 모듈은 이 seed를 입력으로 한다.
인코딩 - 모든 처리 모듈은 처리모듈 아이디를 내용과 함께 차원축소하여 출력 코드로 출력한다.(아니면 출력에 처리모듈 아이디를 lapping하든지)
universl machine - memepute를 universal machine으로 하여 모든 처리 모듈이 입출력 버스를 공유하게 하고 버스는 큐 방식으로 운영하며 버스에 쓸대는 배차적으로 하고 읽기는 각 처리모듈이 
		   seed의 처리모둘 아이디와 버스 스템프를 참조하여 각각 읽기를 독자적으로 수행한다.
신경망의 뉴런 행렬곱의 곱셈을 덧셈으로 하는 것 검토
강화학습 - 1.커널 사이즈만큼 시퀀스를 몬테카를로 랜덤 전개하여 보상에 따라 커널 사이즈 중간을 기점으로 커널 사이즐 반을 각각 이전 시퀀스와 이후 시퀀스로 (소스,타겟)쌍오로 연속적으로 학습시킨다.
	추론 과정에서는 현재로부터 커널 사이즈 반만큼을 입력으로하여 예측 시퀀스를 생성하고 한 스텝씩 전진하며 이 예측 시퀀스가 맞으면 계속 사용하고 틀리면 현재를 기준을 예측하여 확률이 놓므면 
	이 에측겨로가로 수행하고 아미ㅕㄴ mc방법으로 계산하하여 수행한다.
	2.아래 lstm개선 방안대로 수행하며 게임 한판이 끝날때마다 승패에 따라 졌을 경우 높은 가중치 값들을 디스카운트 한다.

lstm 개선 - 커널 사이즈를 두어 과거 기억을(rnn 순환범위) 여기까지만 기억하게 하고 계속 앞으로 진행하면서 커널 윈도우내에서만 과거를 참조하게 한다. 또 커널 사이즈내에서 완전 조합 패스별로 과거 연결한다.
	   각 레렙별로 입력보다 적개 히든 뉴런 갯수를 설정하여 차원 축소되게 하고 이를 계속 상위 레벨로 컨볼루션 한다.
cnn 개선 - 위 lstm개선 사항에서 완전 조합 대신에 랜덤 패스 조합으로 하는것 검토하고 이 조합이라는 것이 곧 cnn의 필터가 됨.
gan 개선 - 위 cnn방법으로 실 이미지를 차원 축소하여 인코딩하고 매번 인코딩 백터를 타겟 망에서 오토 인코더 학습하여 이 두과정을 번갈아 가며 수행, 상위 레벨의 인코딩 백터로 오토 인코딩 수행하면 
	  객체가 인코딩된 타겟 망이 학습됨.

1.시퀀스 인코딩을 사이누어소이드로, 2.조합패턴 생성을 난수 조합으로, 3.커널 바인드 단위 조합 패턴을 rnn을 통과시켜 커널단위 입력을 타겟으로 비지도 학습하여 컨널 사이즈단위 
잠재표현 압축하는 것을 레벨단위로 하여 반복후 적절한 사이즈가됐을대 마지막 레벨은 타겟으로 지도 학습, 4.조합패턴 생성 없이 rnn 없이 시퀀스를 행으로 전치하여 계속 곱하기(
reshape(-1, 2)하여 두개씩 컨볼루션하기) 5.rnn코드 최종 타임에서만 출력, 6.rnn매타임마다 각각 자중치 적용
7.조합패턴 생성 전처리에서 건너뛰는 부분에 0 패딩. 8.6)과 7)을 하여 조합패턴 생성없이 테스트, 9.바인트 조합 결과 압축에 4)번 sequence reduce적용해보기
10.커럴 바인드 조합 단위로 rnn결과를 피쳐로 fully connection하여 압축하기

11.배치압축할때 출처(소스)가 틒린것들을 RN또는 트랜스포머 방식으로 n:n 조합시켜 바인드 배치 구성
12 조합 패턴 생성을 빠르게 할려면 1과 0을 같는 마스킹 패언 조합을 생성하여 place holder로 주입시켜 입력값으로 hardmard dot으로 조합 생성
13.텐서플로우를 C 인턴페이스로 할때 매개변수를 함수 중첩으로 예) axis 함수, dimen 함수 mp.transpose(a, dimen(2,3,4), axis(1))
14.입력 및 목표값 데이터 간에 사이클 계수를 두어 같은 계수끼리 쌍으로 학습, 사이클 동기화로 인해 데이터 입력과 그래프 실행을 비동기로 실행할수있게 됨.
15.실행중에 목표 그래프 활성, 비활성 설정에 따라 해당 목표 그라프로 가는 실행을 자동 drop

16.옵티마이져에서 가중치 갱신을 입력 반복 횟수에 비례하여 조정
17.순전파에서 가중치를 반복횟수(강도)만큼 갱신하여 임계치를 넘으면 다음 연결로 전달(시간적분)하여 학습(비지도 자율 학습)
18.17번에 타겟을 강화학습 개념으로 적용하여 역전파시 긍정 부정 목표값에 따라 연결 가중치 값에 비례하여 증감시켜 기준 역전파 미분에 따른 기울기 소실 문제를 없애는 학습방법

19.기존 역전파에서 기울기 클리핑을 두어 최저값 최고값 각각 하안 상안값을 두어 하한값이하아면 더 작게하지않고 역전파하여 역치 개념으로 기울기 소실 문제 해결.
20.19의 결과 기울기 소실문제를 없애고 가중치 갯수를 적개하고 파동 모양으로 중간층을 설계하여 층 갯수를 많게(깊게)하여 저전력 저사양에서 동작하게 한다.

21.타입 스텝별로 동시 발생하는 이벤트 그룹들을 각 그룹의 신경망 출력 값에 대하여 나머지 그룹의 출력값을 목표값으로 학습시킨다. 또한 동시 발생 그룹이 없으면
   동일 그룹내에서 현 시간 스텝에서 다음 시간 스텝을 출력값을 목표값으로 하여 자가 학습시킨다. 연상 기억 학습, 시간이 중요. 해마

22.연결 시뮬레이션, 동일 타임스텝에 발생된 이벤트들이 반복 임계치를 넘으면 이들을 연결(시퀀스이면 이벤트간 순서도 고려) 비지도 학습, 
   긍정/부정 결과에 따라 종착 뉴런을 역으로 연결을 따라가며 가중치 증감 보상학습, 이벤트 발생 순서에 따라 국부에 메모리에 연결 생성하여 로컬 단위로 gpu에 연결 추론
   연결 아이디를 섹터아이디와 색터내 로컬 오프셋으로 구성하여 색터를 넘어간 연결을 알수있게 하고 연관성이 높은(같은 타임 스텝에 발생) 것을을 같은 색터에 재배치
   각 노드에서 역방향 연결 포인터로 소스를 창아서 값들을 더하여 타겟 노드를 각 노드별 쓰레드가 갱신한다.(블럭이 필요없게) 이과정을 global함수에서 더이상 색터내에 
   갱신이 없을때까지 반복한다. 목표값이 주어졌을때 이 연결망으로 오류 역전파 학습.

   (row % pre_out_size) * suf_shrink_size + row / pre_out_size

23.시간별로 동시각의 입력내지는 신경 출력들을 연결하여 게속 상위 출력을 발생하고 이를 다시 입력으로 같은 것을 되풀이하여 비지도 자율학습을
   하게 하는 것은 완전조하패스 추출기에서 조합패턴을 생성하는 것과 같다, 즉 조합이 동시간대의 신경 출력 연결에 해당한다. 이에 대하여
   지도학습에서는 초기 가중치값을 랜덤하게 주고 역전파 학습에 의해 가중치를 조정하는 것에 반해 비지도학습은 가중치를 초기 아주 작은 값으로
   동일하게 설정하고 반복이 많이 될수록(신호의 세기가 강할 수록) 누적하여 가중치 값을 증가시켜(가장 쎈 가중치가 1에 도달할때까지 1에 도달
   하면 학습 종료, 따라서 반복할때마다 가중치를 아주 적게 조금씩 증가시켜야함) 커널(윈도우)에서 출력값이 쎈 순으로 일정 갯수 패턴만을
   출력시킴. 2차 상위 학습에서는 이 패턴들을 입력으로 같은 학습 방식으로 학습(컴볼루션)하여 압축된(뷴류된) 출력 패턴을 생성, 예를 들어
   32시퀀스 입력, 8사이즈 커널, 2개씩의 출력이면 32시퀀스 입력에 대하여 4번 스트라이드하여 1번 스트라이드에 2개 출력하여 8개의 출력
   생성, 2차 학습은 동일하게 하습시키면 2개 출력 패턴 생성(실제로는 스트라이드를 오버랩하여 적용), 이것은 분류이고 라벨 생성기

23.신경망으로 입력값에 가중치를 곱하여(더하던) 출력값을 내는 방식은 타겟값에 의해 역전파를 할수없다면 그 의미를 둘 근거가 없음, 따라서
   라벨 생성을 mepute 조합패스 추출기를 사용하여 최종 출력된 아디드들을 타겟 심볼로 하고 이것들을 원핫 인코딩한 것을 목표값으로 하여
   신경망에 지도학습 시킨다. mepute 비지도 학습기는 빈도수에 의한 비지도 학습으로 의미가 있으나 discrete한 것디 단점인데 이것을
   라벨 생성기로 사용하여 신경망 지도학습시키면 스무딩 결과를 가져오고 학습되지 않은 라렙도 예측할수 있게됨. 결론적으로 두개 모델의 
   장점을 취하여 타겟 라렙없이 신경망 학습시킬수있는 방법론이 된다.

24.초기 단위 패턴 생성시에 신경망에 의해 가중치 조정방식으로는 적절한 입력-타겟 쌍이 없이는 패턴을 생성할수없다. 순환데이터의 경우 적절한 
   시퀀스 시작점도 설정랄 기준이 없다. 시작점이 중구난방이면 입력값이 모두 썩여 조정되는 가중치 값이 전체 평균값이 되어 버리기 때문.
   따라서 이러한 상태에서는 조합패스 추출기만 한 것이 없으며 비지도 방식으로 초기 패턴을 형성하는데 
   조합패스 추출기를 사용한다. 추출기는 있는 상태 그대로 타겟과 가중치 조정 개염없이 비지도 방식으로 패턴을 형성할수있기 때문이다.
   일단 커널 사이즈내 초기 단위 패턴을 추출기로 생성후 같은 단위 패턴를 지적하는 단위 입력 시퀀스들 별로 타겟을 참, 거짓으로 가정하고
   참값으로 타겟 설정하여 역전파 학습시킨다. 이 과정을 상위로 반복, 23번과 차이점은 컴볼루션 각 레벨에서 추출기를 사용하여 학습한다는 점이고
   포인트는 추출기로 타겟을 생헝하여 역전파에 적용 결국은 비지도 학습으로 분류문제를 해결한다는 것.

25.anet에서 expand에 의해 생성된 바인드 조합 시퀀스들중 쓰레기 조합을 추출기에 조회하여 pruning하여 일정 갯수만을 bind하여 배치로
   구성해서 학습시키는 방안.

26.주가예측과 같이 순환 시계열 데이터에 대하여 시퀀스 시작점을 추출기로 조회하여 가장 빈도수가 높은 단위 패턴이 있을때 이것을 시작으로
   시퀀스를 조정하는 방안. 이렇게 검색하는 것을 추출기가 아니고 24번으로 학습된 expand가중치 조합 bindbatch시퀀스에 입력을 
   스트라이드하여 곱해서 가장 출력값이 큰 입력 시퀀스를 찾아내는 방안.

27.자가 생성망- 가중치가 입력값을 모사하게 학습시킨다. 방법은 반복되는 입력에서 각 가중치의 이동평균을 구하고 입력을 이동평균으로 나누어
   1을 기준으로 남으면 남는 만큼 뺀값을 모자르면 그대로한 값(w1)에 반복횟수에 일정계수를 곱하여 누적 반복횟수에 비례한 값(w2)를 곱한 값을
   출력으로 한다.(역전파 필요없음) 같은 입력 시퀀스 번째에서 이전과 다른 값이 나타나면 새로 뉴런을 생성하고 이전것들과 경쟁한다.
   decay를 두어 각 뉴런별로 최고 출력 대비 20%이하이면 반복 카운트하지 않고 반복 카운트가 decay되어 0가 되면 뉴런을 삭제한다.
   뉴런의 출력이 50% 이상에 도달한 것들끼리 2차 시퀀스를 구헝하고 이 시퀀스에 대하여 anet에서 expand에 의해 바인드 조합 패턴 뉴런들을
   생성한다. 이렇게 생성된 뉴런들에 대하여 decay를 적용하여 계속 살아 남은 뉴런들을 대상을 시퀀스 구성하는 것을 반복한다.
   학습 종료시까지 이과정을 반복하고 학습 종료 후에는 입력에 대하여 최고 출력을 하는 뉴런이 분류 결과이다.
   각 뉴런이 생성된후 점차적으로 이동 평균 변화량을 적게 가져가고 일정 시점 이후에는 더이상 학습되지 않게 한다.
   decay는 입력 회차를 기록하여 입력 뉴련 또는 2차 생성뉴련들이 각각 차신이 생성된 시점의 회차를 보관하여 자신이 생성된 이후브터 최종 
   회차 순번까지 몇번 발현되었는가를 뷴율로 한다. 에료 23회차에 뉴련이 최조 생성(입력)되었고 최종 회차가 100이며 매번 발현됬다면
   77 / 77, 20번 발현됬다면 20 / 77, 70회차에 생성되어 10번 발현됐다면 10 / 30 이고 이 decay가 일정 비율 이상인 것들을 
   stage에 올려 놓는다. 입력단을 시작으로 1차 스테이지에 올라온 것들을 커널 사이즈로 스트라이드하여 2차 스테이지에 올리고 1차 스테이지
   에서는 제거하면 1차 스테이지상에서 커널 사이즈 범위를 벗어난 뉴런들이 가까워지고 이 시퀀스등를 대상으로 커널 스트라이드 하여 2차 스테이지
   롤 올린다. 이 과정을 스테지에있는(게속 decay를 넘어서 상주되는 것들) 뉴런들이 모두 커널사이즈내에 들어올때까지 계속 한다. 상위 스테이지
   에 올린때는 멤버 뉴런들의 (하위)스테이지상에서의 시퀀스 아이디를 근거로하여 상위 스테이지의 순번을 산정에서 삽입한다. 이렇게하여 입력 시퀀스
   의 모든 뉴런들이 decay를 거쳐 조합될수있도록 한다. 이전 버전은 인덱스 복합키를 구성하여 조회하여 뉴런릉 타겟팅 했던것을 
   뉴런을 구성하는 하위 뉴런 시퀀스가 있을때 그들의 첫번재 뉴런부터 시작하여 그 뉴런으로 시작되는 다음 뉴런들을 avl이덱스로 연결하여
   뉴런에서 다음 시퀀스 뉴런을 avl인덱스 검색으로 찾게한다. 즉 각 뉴런을 구성하는 하위 뉴런의 첫번째를 시작으로 작은 avl인덱스들이 각각
   있는 것이다. 하여 입력 시퀀스가 주어지면 커널 사이즈로 1칸씩 스트라이드하며 패턴 조합 추출기가 뉴런에서 뉴런 연결이 avl로 되있는 것을
   각 avl을 검색하며 진행한다. 시퀀스 인덱스는 커널 사이즈내에서만 유지하여 자리를 구헝하고 그 위치를 기반으로 뉴런 패턴 시퀀스를 생헝하고
   상위 스테이지로 올릴때가 되면 주어진 입력시퀀스로부터 커널 스트라이드로 추출된 패턴들을 상위 스테이지에 순서대로 올리고 이것을 다시 입력
   시퀀스로하여 상위 스테이지의 커널 사이즈내 위치를 기반으로 avl인덱스 연결을 한다. 즉, 생성할때는 전체 시퀀스가 아닌 커널내 위치로만
   하고 추출할때는 입력시퀀스로부터 그 시퀀스대로 추출하여 전체 위치상 나열한다. 따라서 최 상위 스테이지 뉴런이 전체 시퀀스를 함의하게되고
   결과 순환데이터나 언어과 같이 위치가 가변인 데이터에 대하여 시작 시퀀스를 어떻게 정하던 상관없는 패턴 뉴런을 생성할수있다. 
   상위 뉴런으로 출력되는 신호의 강도는 하위 뉴런 신호의 decay된 강도 합을 멤버 뉴런 갯수로 평군하여 출력하므로 출력 강도는 아날로그이고
   상위로 올리는 강도 커트라인을 하위 뉴런들중 50%을 올리므로 최종 1개을 선택하게 하고 이것이 분류 결과이다. 
   구성 뉴런의 갯수에 따라 가중치를 주지은 않는다. 무조건 바로 아래 하위의 강도합으로 한다. 뉴런의 갯수가 작은 것이 크로스엔트로피 정보이론
   에서는 더 에너지가 높은 것도되므로(특이한 것) 

28.중요한 것은 완전 조합 패스 탐색은 자리(위치)를 기반으로 하는 것이다.값이 아니라 따라서 조합당 하나의 탐색만 존재하며 탐색을 단일하게 
   한다. 그리고 신경망 연결을 소프트웨어로 시뮬레이션하는 것이 경우의 수로 불하능하나 윈도우 커널 사이즈와 위치 기반 완전조함 탐색 방법론에
   , 그리고 각 분기노드에서 생성되는 avl 인덱스에 의하여 가능하게 된다.
   자리(위치)는 터미널이 피쳐(디멘젼)별로 선택된후 이것들의 입력 시퀀스가 구성된 다음의 개념이다. 터미널은 연속형 아이템은 27.번 설명과
   같이 근사 추정값(완전일치면 1, 근처 값이면 소수점 이하값)이고 distcrete값이면 검색하여 일치하는 아이템 값 1로하고 이렇게 선택 추정된
   값에 반복 가중치를 곱하여 각 아이텝의 입력값을 구성하고 이 아이템 아이디로서 시퀀스를 구성한다. 이후 이들 입력 시퀀스를 앞서 설명한데로
   위치(순서)에 기반하여(위치 인덱스를 조합 정보에 포함시키는 것은 아님) 조합 패스를 구성하고 이 조합을 또 다시 하나의 단말로서 처리하는 
   것을 상위로 계속 반복한다.
   디멘젼은 디멘젼(피쳐)인덱스 별로 인덱스를 구성하고 터미널을 선택하기위해서만 필요하다. 이렇게 선택된 터미널들이 시퀀스 구성될때는 디멘젼 
   구분없다.
   구성 뉴런의 갯수와 스파이크 볼트 평군간 상관계수 설정이 관건.

28.접압 세기는 발화와는 무관하다. 발화는 같은 접압이 반복됐을때 발화한다. 같은 위치에서 접압세기에 따라 같은 전압이 반복될때 분화하고 이것은
   상위에 디벤젼을 구성한다. 발화는 수평 발화와 계층 발화가 있으며 수펑발화는 조합이며 같은 계층에서 주어진 시퀀스를 따라 완전조합 방식이 
   아닌 점증 조합, 그러니까 현 조합에서 계층 발화후 시퀀스상에서 다음 인접 노드로 조합을 확장하고 이것이 반복되지 않으면 이번 확장 노드를
   버리고 다음을 계속 반복하여 탐색해 나가는 분화이다. 게층발화는 시냅스 반복이 임계치에 도달했을때 다음 상위 계층으로 분화하는 것이다.
   값의 범위에 따라 분류기에 의해 가지수(아이디)로서 시넵스에 전달되므로 역치를 넘어서서 먹는 것은 발생하지 않으며 하위 연결조합의 모든 뉴런
   의 출력값을 더한 값에 반보계수를 곱하여 그대로(relu 활성화 방식) 발화값으로서 출력한다. 이렇게 발화한 상위 시넵스의 입력값은 그 범위에 
   여라 또 가지수가 된다. 시냅스 연결은 이 가지수의 조홥이며 분류기는 애매한 값을 그 양쪽 범위 두군에에 전달하여 일치율에 따른 계수를 곲하여
   (정확히 일치하면 1, 아니면 소수이하)적젉값이 두군데로 전달됙게 한다. 지도학습을 하고 싶으면 이 연결을 역전개하며 미분하면 된다.
   그리고 하위의 가지수에의해 상위 시퀀스의 디멘전으로 구성된 것들은 상호 배타적으로 발생하므로 수평 발화할때 동시에 발화할일 없음으로 수평 조합
   확장시에 선택된 디멘젼 번째 이외 동일 디멘젼상의 것들은 건너띈다.

29.입력은 피쳐(여러개의 엘레먼트 집합)으로 구성되어 어떤 특정값으로 산정하지 않는다. 따라서 시간 순서에 따른 시퀀스를 구성하는 각 입력의
   피쳐를 완전 조합방식으로 매스킹하여 나온 패턴들을 병렬로 28.에서 설명한 방식대로 시퀀스를 진행하며 수평 발화와 계층발화를 수행한다.
   이렇게 하면 매우 일반적(범용적)내지는 짧은 매스킹 패턴은 시퀀스 전반에 걸쳐 광범위한 수평 시간적분(시냅스 길이가 긴)을 가질 것이고 
   특수한 또는 긴 패스틴 패턴은 좁은 범위의 수평 시간적분을 가질 것이며 이 모두가 시퀀스 하나 하나의 입력에서 각 특성끼리 조합되는 다중 
   트위스트 시간 패턴 조합을 수행하게 된다. 번역의 예를 들면 원문과 번역문을 쌍으로 입력하여 원문에서 -> 번역문의 순서로 통합 시퀀스 구성하여
   수평, 계층 발화를 수행하고 평가시에 번역문의 어순에 따른 시퀀스 맞춤(정렬)은 학습시 번역문 파트에서(원문파트도 마찬가지)시간 적분될때
   (시퀀스 순서에 따라 조합 패턴이 구성될때) 어순상 앞쪽은 독립적이고 뒤쪽은 시냅스 구성이 앞쪽의 발화 결과 후에 뒤쪽이 연결되게 되므로
   즉, 시냅스이 좌측이 어순상 앞쪽 우측이 어순상 뒤쪽이 되어 어순 순서대로 출력되게 된다. 평가때는 원문을 발화 방향으로 흐로고 번역문
   파트는 학습때 발화의 역방향으로 흐르게 하여 시냅스 좌측이 발현되었고(다른 시냅스에 의해서) 시냅스가 발현되엇으면 이 시냅스의 우측을
   발현하여 이 과정이 연쇄적으로 수행되므로서 번역이 완성되게 된다. 이때 매스킹 패턴에 의해 병렬로 수행된 결과 각 시퀀스 자리에서 발현되는
   매스킹 패턴들의 교합이 가장 많이 되는(hit rate가 가장 많은) 단어를 선정한다. 각 시퀀스 자리를 특정하는 방법은 첫번째로 발현되는 
   피쳐 매스킹 간에 시간적분상 선후가 있는 지를 파악하여 선이 없는 것들끼리 경쟁(또는 합)하여 발현하고 두번째도 마찬가지로 두번째로 
   발현되는 것끼리 이 과정을 수행한다.

30.29.에서 피져 조합별로 병렬 진행하지 않고 피쳐 전체 하나로 하면서 피쳐중 일 부분이 시냅스 전후로 반복되면 연결시키는 방안. 피쳐가 
   1개 값이 라도 이를 범위로 구분하여 일정 범위별로 복수의 피쳐를 구성하여 동일 하게 진행. 이렇게해도 각 시퀀스에서 피쳐의 부분부분이
   틀리게 매칭되므로 병렬로 수행됨.

31.수평시간 적분에서 분기됐다 한 두개 정도 진행된후 분기된 것들의 후행이 동일한 경우 이를 하나로 합쳐 일종의 or조합을 생성한다. 이러한
   분기들은 하나의 시퀀스상에서는 동시에 존재하지 않는 배탓적인 것들이고 이것들이 or로 묶여 하나의 시간적분 조합 패턴이 됨으로써 넓은 
   범위의 조합 생성을 가능하게 한다. 어러한 조합 패턴은 문법과 같은 형태를 가지게 된다.(문법보다는 좀더 세부적인 의미기반이되는)
   어떤 패턴은 분기부분이 다른 패턴은 고정부분이되고 이러한 것들이 계속 시간적분 패턴되며 다중 중첩 패턴들을 형성하게 된다. 또한 주어진
   시퀀스상 일정 스텝(1개 내지는 2개)를 건너뛰게되면 분기패턴을 생성하지않고 원문의 한 부분에서 원문의 그 후속 전체를 건너뛰어 번역문의
   한 부분과 시간적분되는 패턴(1)도 생성될수있고 번역 평가할때 원문에서 번역문으로 이어지는 시퀀스에서 전체 연결을 포괄하는 분기패턴을 
   포함하는 골격패턴(형식또는 의미문법 패턴)중 제일 강도가 쎈것이 선택되고 이 패턴의 분기들을 (1)과 같은 패턴들과 함께 패턴들의 교합이
   가장큰 단어들이 분기부분에서 선택되게 된다. 계층 발화는 수평 발화 시간적분 패턴들이 공통적으로 같은 부분에서 연결이 건너뛰거나 분기가
   매우 많은 부분이 있을수 있고(연결성 희박구간) 이럴때 더 이상 수평 시작적분하지 않고 상위 계층으로 마지막 뉴런들을 발화시킨다.
   상위 계층에서는 다시 이것들을 입력으로하여 수평분화를 수행하기를 계속 상위로 반복한다. 이렇게 분기 패턴을 포함하는 수평시간 적분 패턴이
   상위로 발화함으로써 분기부분이 or조합됨으로써 상위 게층에서 긴 수평적분을 수행할수있고 상위로 가면서 추상화 될 수있다.
   분기는 서로 다른 시간대(다른 시퀀스)간에 발생하는 것들이 or 조합됨으로서 긴 수평시간 적분이 가능하게 하고 평가할때 입력 시퀀스상에서
   학습때는 없었던 조합의 해석을 가능하게 한다.
   피쳐는 개별을 그대로 시퀀스로 늘어놓던지 피쳐단위를 먼저 적용하여 피쳐 아이디 생성후 이 아이디로 하던지..

32.구현방안 - 매 시퀀스 입력마다 한단계씩 진행(예들들어 2번 이상 반복될때 다음 스텝 패스 형성)하며 일정스텝(예로 2개 스탭)이상 중복되면
   이게 아니고 무조건 중복회피(이렇게 하면 완전조합과 같게될것)
   중복을 회피하기위해(현 패스는 그대로 진행하고) 시퀀스상 다음 스텝으로 점프하여 분기, 이렇게 중복을 회피함을로써 다양한 패스(완전조합
   과 같은)를 형성하게됨. 일정 시간(배치를) 이렇게 학습한후 조정 절차 진입, 조정절차는 적당한 분기 구간(예를 들어 스텝길이 2개, 분기
   20개 정도?)훅에 합류되면 합류지점을 묶어서 하나의 패스로 하고 분기가 그 이상이면 끊고 합류지점을 기점으로 묶어서 후행을 진행.
   이렇게 끊어진 마지막 종점들을 상위로 올려 이헐게 상위로 올려진 패턴들에 대하여 동일 과정 상위 반복. 하위에서 끊어진 부분들 포함하는
   다른 패턴 조합들이 상위로 올라감으로써(병합에의히 or조합으로) 상위에서 패턴 조합될수있는 여지가 생기고 상위 패턴이 형성됨.
   재현과정은 최상위까지 선택된 패턴들을 하위로 전개하며 맨 하위 패턴에서 시퀀스를 중첩시켜주고 분기가 발생되면 가중치가 높은 것을 선택하여
   단일 시퀀스를 생성함. 번역과정은 최종 선택된 양측 두개패턴을 하나의 패턴으로 연결하여 재현 과정을 수행하면 번역이 완료됨.
33.위 32에서 합류지점을 묶어서 하나의 패스로 하는 것을 합류 유형들을 입력으로하고 합쳐지는 하나의 패스를 타겟으로 하여 오류역전파
   지도학습 시키는 방안.
   
34.내부 반향(reflection)과 사이클 - 내부 경험이 다시 모든 외부 이벤트에 추가되어 같이 학습됙고 사이클이 이루어지면 사고가 생성.
	비 학습 입력에 대한 적절한 반응을 가능케 함.
35.상위 10% 정도의 빈도수를 갖는 워드를 기능어로 하고 나머지 90%중에서 단락내의 문장간에 중복되는 단어들을 익명 전처리 하여 하습시키고
   후처리에서 이 익명을 실제 워드로 치환한다.
   단락 단우로 잠재코드를 오토인토딩 하여 생성하고 문장단위 연결 학습에서 단락단위의 잠재코드를 내부 반향 개념으로 바인드 배치에 추가하여
    문장단위의 다음 문장 예측 훈련시킨다.
   평가 단계ㅔ서도 마찬가지로 단락단위의 잠재코드 예측을하고 이 예측된 잠재코드를 바인드 배치에 추가하고 문장단위 다음 무장을 예측한다.

36.분산 학습 - 배치 파이프라인 방식으로 분산 학습한다. 실행 그라프의 처리 순서상 각 단계별로 한대의 기계를 할당하고 각 기계는 
	해당 단계 처리에 필요한 가중치만을 갖는다. 각 단계 기계별로 가중치를 소유하며 각 기계는 자신에 할당된 각 단계만을 처리한다.
	배치를 하나씩 첫 단계부터 흘려 보래고 연속하여 오버렙되게 다음 배치를 플로우에 태운다. 역전파를 거쳐 처리 플로우의 마지막에서 
	이정 배치 단위로 취합하여 가중치를 업데이트 하고 이과정을 반복한다. 이때 가중치 모든 단계의 가중치를 한꺼번에 변경하지 않고
	이전 수행결과 업데이트 후 첫번때 선두 파이프라인이 해당 플루우 단계의 가중치를 읽기전에 단계별로 업데이트 한다.
	동기 방식으로 배치를 동시에 실행하고 취합하지 않기 때문에 지연이 없고 비동기 방식도 아니기때문에 지연된 기울기 문제도 없다.
	각 기계는 각 단계별 가중치만을 유지하기때문에 메모리 낭비가 없으며 행렬을 수직으로 분할하여 처리하는 분산처리에 비해 복잡성이
	전혀 없고 여러 동기화에 소모되는 사이클이 없다.

37.용어 - induction, neutral(중립), coaxial, relay, arrester(방지장치), bipolar, tachometer, amplifier, 
	condenser, reactor, vibrator, regulator, rectifier(정정자, 정류기), shunt(옆으로 돌리다), 
	solenoid(관형코일), trip coil, tumbler

38.오토 인코딩(자기복제 학습)하여 잠채코드를 추출후 잠재코드들 간에 연결학습을 수행하면 몇개의 셈플 연결 학습만으로 전채의 연결 학습
	결과를 얻을수있다. 이유는 자기복재 잠재코드는 일반화가 수행된 것이고 따라서 일반화 잠재코드끼리 연결학습을 하면 일반화된 연결 학습
	결과이기때문에 학습과정에서 연셜학습하지 않는 개체들도 평가단계에서 자기복제 네트워크로 잠재코드를 생성하여 잠재코드 연결 네트워크에
	추론하면 그것이 곹 일반화된 결과이다.
39.화자인식은 일반대화를 컨볼빙한 타겟에 화자만을 연결학습하면 화자의 목소리 특징을 판별하는 네트워크를 만들수있고 이 네트워크에 평가 단계에서
	학습되지 않는 화자도 그 목소리 특징만을 출력할것이고 이것으로 화자를 구분하면 된다.
	같은 화자냐 아니냐를 판별하는 네트워크는 동릴 화자로 출력되는 결과가 같으면 보상을 아니면 퍁널티를 주어 같은 화자일때 하나의 값을
	출력하게 하고 이러게 훈련된 네트워크에 다를 화자를 평가.
40.스트림 언어와 네임포트 - 네임포트는 로컬/원격간에 텐서를 송수신 적재하고 임의 네임포트를 리슨하는 수신대기 액터들은 텐서가 도착하면 
	활성화되어 작동, 스트림의 각 마디들은 각각 목적하는 coaxial망를 트래이닝/평가하는 주체, 스트림은 신경망이 해석/생성 하는 언어가 되고 
	전체 망은 스트림 언어로서 스스로 작동. 임의 스트림의 마디들은 묵시적인 하나의 네임포트를 사용하여 메개 텐서를 주고 받으며 순차 실행
	스트림언어는 루프나 제어구문과 같은 문법이 없고 네임포트를 통한 순환 과정에서 사이클이 형성되어 작동.
	시스템의 내부 작동은 메모노드 데이터베이스를 사용한다. 원시 데이터는 발생 시각과 장소로 인덱싱, 망에 의해 파생되는 데이터는 원시데이터의
	발생시각을 상속받고 학습 소스와 타겟을 일정길이 시퀀스로(키 구성길이) 코엑실 학습 압축하여 키를 생성후 데이터베이스 시스템의 인덱싱에 등록
41.타겟 - 예측 시스템 - 키스트림, 압축학습 등은 시퀀스 패턴이고 시퀀스 패턴은 중첩되는 조건이다. 따라서 이는 정보를 충실히 구성하는 객체로서 이를
	데이터베이스로 키로서 인댁싱하고 예측하고자 하는 소스와 타겟 도메인의 과거 발생한 데이터를 쌍으로 압축 학습하여 키를 생성후 이 키로서
	검색하여 예측하고자 하는 도메인과 유사한 도메인을 찾고 예측하고자 하는 도메인 소스와 검색한 도메인 타겟을 연결 학습하고 예측하고자 하는
	도메인의 현재 이후를 예측한다.

42.연결 시스템 - 로우 데이터는 파일 시스템을 사용하고 종류 및 상하위 바인딩 정보들을 데이터베이스로 관리한다.
43.훈현 아키텍쳐 - 모든 종류별로 데이터를 레렐별로 자기구조학습(오토인코딩)훈련 시키고 상위 레베은 하위 레벨의 훈련이 완성된후 
	하위레벨을 평가한 값들로 시퀀스를 구성하여 가지구조학습 시키는 것을 상위로 반복한다.
	언어의 경우 문장단위 학습레렐이 맨 하위이고 언어를 제외한 자연계의 데이터는 임의의 일정 길이 단위를 맨 하위로 하여
	학습시키고 그 상위 레벨의 자기구조 학습부터는 
	방안 1.입력시퀀스를 일정 부분씩 오버렙하여 학습시키고 재생또한 동일한 오버랩을 적용하고 앞선 시퀀스 재생에서 맨 끝에 부분은 
	후행 시퀀스의 오버랩 부분으로 대체한다. 경계 부분이 의미 단위에 맞지않게 잘릴수 있기때문이다.
	방안 2.맨 하위 레벨만 레벨 단위로 선 학습 완료후에 그 상위 레벨부터는 최상위까지 coaxial에서 오버랩 옵션을 주어
	한꺼번에 학습시킨다. 이렇게 하면 의미 경계부분이 자연히 해결된다.
45.long시퀀스 학습 - 
	1.타겟시퀀스 자기구조학습(오토인코딩)
	입력시퀀스를 64길이단위로 오버렙하여 스트라이드 하면서 오토인코더로 학습하여 8길이 잠재 코드 생성,
	하위에서 생성된 8길이 잠재코드를 다시 입력으로 상위 레벨의 입력시퀀스를 구성하여 반복, 시퀀스 길이 64가 되면 최종압축
	잠재코드는 8길이가 되고 인코딩 종료, 이렇게 전과정이 연결되어 오류역전파 되지않고 각 레벨단위로 역전파 단절하여 평가의 결과를
	상위 입력으로 하므로 오차가 중첩되어 최종 정확도가 떨어질것 같으나 이러한 오차역시 하위레벨 학습이 끝난상태에서 상수로서
	상위 레벨에서 입력되어 다식 학습되므로 상관없다. 또한 오토인코딩이므로 하위에서 생성된 잠재코드중에서 오차의 일정 이하를 
	필터링 하여(입력 잠재코드 시퀀스를 타겟으로하여 오차 측정) 전체를 통으로 역전파 하는것보다 정확드롤 더 높힐수있다.
	디코딩 과정은 맨 하위를 제외한 레벨의 디코더 출력을 입력->출력을 반복하여 시퀀스를 생성하고 맨 하위는 이 시퀀스와 
	원 타겟 시퀀스를 연결 학습 시키고 예측때 이 연결 학습의 결과를 출력한다.
	2.입력시퀀스-타겟시퀀스 인코더 최종압축코드 연결 학습
	타겟시퀀스 인코더 최종압축코드는 위 1)방법으로 생성하고 입력시퀀스를 1)방법으로 64길이까지 시퀀스를 압축 생성한후 
	이 시퀀스를 1)의 최종압축코드와 연결학습시킨다.
46.연결 시스템 - coaxial, stratus망을 실행을 각 망별로 복수로 별도의 프로세스로 실행하고 제어프로세스에서 이 망프로세스로
	IPC로 플럭스를 송수신하여 실행한다. 이유는 쿠다 아키텍쳐의 gpu병렬 실행이 프로세스 단위이기때문이다.
	제어프로세스는 에이젼트(세션유지)와 이안에 망 실행 파이프라인 정보, 그리고 각 망에 실행 요청하는 쓰레드 시스템, 
	요청스팩등으로 구성되며 나중에 네임포트, 네임포트를 대기하는 에이젼트들의 관계, 이의 데이터베이스 등록 및 관리등을 
	추가한다.
47.닮은꼴 보기(연예인 닯은꼴), 가상 캐릭터 생성 - 학습후 학습되지 않은 입력으로 디코딩
48.검색시스템(이미지,사운드 검색엔진),닮은꼴 찾기 - 8바이트 최종압축성코드부터 비교하여 동일 압축코드 그룹에서 하나 하위 레벨 세부 압축코드 비교로 브렌치하여 인덱싱하고 트리 검색, 
	- 이미지 검색을 넘어서 카메라를 눈으로 만드는 것, 카메라를 키면 현 장면의 검색이 자동으로 이뤄져 정보 제공, 저건뭐지 하는 순간에 검색 정보제공, 라벨이 필요없음
49.학습,에측시스템 - 쉽게 모델 만들고 간단히 입력, 학습 및 예측 수행.
50.강화학습 - 정해진 시퀀스 길이를 예측하여 실행하고(맨 처음에는 노이즈로부터 생성되고 이후에는 이전까지 학습된 것으로 예측된 결과)
	이전 보상평균 이상이면 이 시퀀스를 기억하여 학습시키고 어 낮은 것은 버린다. 이과정을 반복, 그리고 보상은 이전에 학습된 다른 
	망의 예측 결과일수 있다.

51.디코더 과적합 방지 - 행렬 외적의 로우를 길게 하고 컬럼을 짧게하면 컬럼에 곱해지는 가중차의 사이즈가 줄어 가중치 공간이 적어지므로 과적합이 방지된다.
		이러한 방식으로 디코더도 코엑실로 압축하여 풀어주는 방식 고려

52.시퀀스 순차 학습및 예측 방법 - 최하위 레벨부터 쵀대 8개 레벨을 두어 하위에서 8시퀀스 길이로 모자르는 부분은 제로패딩하여 
	오토인코더 학습하여 1개 압축 코드를 생성하고 8길이가 됐을때 그 압축코드를 상위 레벨에 적재하고 그 상위도 같은 방식으로
	압촉코드를 생성하여 그 상위로 올린다. 8 레벨의 각 레벨에서 생성된 압축코드를 모으면 8길이가 되고 이를 시퀀스 입력으로
	다음한개롤 목표값으로 하여 최종 연결학습하고 이 목표값을 다음 입력으로 하여 이 과정을 하나의 에피소드 또는 
	원문-번역문쌍 시퀀스가 끝날때까지 반복 학습하고 예측도 동일한 방식으로 한다 또한 이것은 다이나젠과 틀리게 디코더 부분을
	추가 학습 시킬필요없다.(복원과정 자체가 없으므로)
	메모리 뱅크 설계 - 최하위 1 레멜은 입력 토큰 8개, 상위 2레벨은 1레벨 8개를 코엑실로 압축한 1개 코드 8개 적재
	이러한 방식으로 상위로 반복하여 최대 8개 레벨 뱅크를 두고 최하위 뱅크는 8개 입력토큰 그 상위 뱅크는 8개 압축코드로서
	뱅크당 8개씩 최대 64개 시퀀스를 구성하여 코엑실로 압축하여 1개 토큰을 예측, 원문과 번역문 문장 길이를 32로 할 경우
	2레벨 뱅크 4개 원문 압축코드와 1레벨 뱅크에 입력토큰 1개를 시작으로 휸련/예측, 2레벨 뱅크에 원문/번역문 각각 압축코드
	4개하여 8개 적재 그리고 입력토큰용 1레밸 뱅크하여 2개 레벨 뱅크만 있으면 됨.
	학습할때는 입력 8개 토큰단위로 1리벱 뱅크와 압축하여 상위로 올리는 작업을 하며 학습한다, 즉 8개 입력토큰 단위로 학습한다.
	필요하면 입력 마스킹하여 후행은 0로 처리하는 작업을 하나 사실 anet에서 모든 경우를 완전 조합으로 학습하니 필요없지 않을까.
	하여간 마스킹을 하지 않으면 한번에 매칭하는 단위 위 8개 토큰은 양방향이 된다.

53.자동 레이블러(rabeller)비지도학습 자동 분류 - 다이나젠으로 단말 오토인코딩 학습후 최종 제너릭에서 분류갯수로 크로스엔트로피 분류 결과 
	고르게 분포되는 1차 셈풀을 추려서 비지도 분류된 결과(노이즈-가중치로부터 분류) 가지 넘버링을 타겟으로 지도학습, 나머지 셈플들로 1차 
	지도학습된 제너릭망으로	2차 비지도 분류하여 고르게 분포(대표)되는 2차 셈플을 추려 분류된 넘버링을 타겟으로 라벨일된 1차 셈플 과 합하여 
	다시 지도학습, 이를 모든 셈플이 분류될때까지 반복, 레이블망을 글로벌로 하지 않고 현 상태 레벨(첫번째 문장 1레벨, 두번째 문장 1레벨, ..)에서
	다음상태 레벨에 오는 문장들만을 대상으로 레벨별로 레이블망을 구성, 현제상테에 도달하기까지 경유하는 상태아이디 패스와 현 문장을 입력으로
	다음 문장을 타겟으로 연결학습하고 타겟문장을 압축인코딩후 다음 상태레벨의 레이블망에 분류학습하여 분류하고 그 분류된 상태에 타겟문장을
	정답으로 등록, 이때 경로상 이전 상태 레이블망들에도 타겟문장의 압축코드를 추론하여 정확도가 높게 나오면 그 상태로의 상태전이 연결을 설정하여
	추론할때 이 전이 링크의 상태를 다음 상태로서 레이블 분류 조회하게 한다. 분류정확도는 분류가지중 확류이 높은 하나와 다른 것들의 대비 차이가
	클수록 분류 정확도가 높은 것, 추론할때도 경로상 이전 상태아이디들과 현 문장으로 다음 문장을 추론하고 추론된 문장을 압축인코딩하여 현상태 
	다음으로 올수있는 상태전이 링크들을 따라 다음 상태들의 상태레벨 레이블망(레이블망은 상태 레벨별로 공유됨)에서 분류조히하여 가장 정확도가 높은
	상태로 전이하여 그 상태의 정답 라이브러리중에서 가장 코사인 유사도가 높은 것을 정답으로 하여 다음 문장으로 선택하고 이를 다시 현문장으로
	하고 위 과정을 반복하여 계속 다음 문자을 추론해 간다. 이때 이전 상태 아이디 시퀀스는 pcode(previous압축코드)로서 정제된(필터링된, 분류된)
	이전 문맥 기능을 하여 분먕하고 정확도 높은 다음 문장을 추론할수있게 한다.

54.수동 레이블러 - 문장을 도메인 의미적으로 비슷한 것끼리 버켓리스팅, 이 버켓 가짓수를 목표값으로 문장:버켓넘버 지도학습
	버켓 아이디별로 동류 문장들을 담아놓은 정답 라이브러리 구축
55.자동 SFA(semantic finite automata) 생성 - 응답 문장을 맞물려 가며 추론하고 각 추론된 문장을 53 또는 54 로 생성한 레이블 망에
	질의하여 버켓아이디와 해당 버켓에서 코사인 유사도로 가장 일치하는 정답문장을 발췌하여 다음 질의 문장으로 하여 반복하여 플로우를
	SFA로 구성, 마지막 상태가 예약과 같은 업무 종착으로서 bussiness triger발생, 문장에서 숫자나 고유명사는 전처리에서
	태깅하여 매칭하고 마지막 상태에서는 이 태깅에 해당하는 숫자, 고유명사로서 비즈네스 로직에 키값으로 사용한다. 또는 상태 머신에서
	인위적으로 빌더에서 스텝을 추가하여 키값을 획득한다. 비지네스 로직에서는 네비게이션 상태 패스 아이디와 키값으로서 로직을 처리한다.
56.지식베이스 - 레이블망과 응답연결망, SFA로 지식베이스가 구성된다.

57.추론엔진 - 1. 원반적으로 세부단위의 타겟이 주어지는 것은 인위적으로 작업하지 않는 이상 어렵다. 반면 일련의 시퀀스 뭉치/블럭 단위의 사건은
	실생활에서 자연스레 그것의 결과가 도출되어 시퀀스 뭉치 단위의 타겟 상태가 될수있다. 이 아이디어는 뭉치단위의 타겟으로부터
	그 뭉치내의 시퀀스들의 타겟을 도출하여 세무 시퀀스를 학습시키는 것이다. 뭉치의 타겟 상태를 그 뭉치내의 세부시퀀스의 압축 잠재코드
	의 타겟으로 뭉치내 모든 각 시퀀스에 대하여 뭉치단위 상태를 동일 타겟값으로 복수로 매칭하여 오토 리그레션 방식으로고(각 시퀀스에서 
	이전 시퀀스 잠재코드(시퀀스단위)까지 보는)훈련을 시킨다. 후에 추론 과정에서 연속되는 시퀀스단위별로 각 시퀀스의 타겟의 타겟 상태가 
	추론될 것이고 이것은 몇개의 시퀀스만으로 결론을 앞질러서 보는 효과를 발생한다.(타겟 상태는 실세계에서의 결론이므로) 추론엔진은 
	시퀀스 -> 상태로 이동하고 그 상태에서 다음 입력을 보고 또 다음 상태로 이동하며 추론한다.
	키포인트는 시퀀스의 구성 엘레먼트의 타겟값으로 동일 상태값을 중복하여개 시퀀스를 구성하여 연결학습시킨다는 것(킴반적으로는 
	상태 목표값 1개를 연결 학습 시킴) 그리하여 추론 과정에서 오토리그레션 방법으로 입력 시퀀스의 엘레먼트별로 앞에서부터 순차적으로
	매 엘레먼트마다 상태값을 추론을 하고 이로서 상태 전이한다.
	2. 빈두수 일정 이하(숫자, 고유명사등을 포함한) 워드를 블랭크 처리하고 이외함께 이의 앞뒤 단어를 묶어서 하나의 워드로 등록하여
	삼단 논법과 같은 논리 대화 학습, 후처리에서 블랭크 단어 매칭 복원, 

58.dual encoder에서의 오토 리그레션이 아닌 인코딩 학습 또는 사적 학습이란 [입력 + 타겟] 쌍을 입력으로 하여 입력 그대로 [입력 + 타겟] 쌍을
	목표값으로 하여 학습함을 의미 (데이터 구성에 go token, end token을 삽입하여 이들 토큰에 의한 추론없이).
	문장 중간 중가에 빈칸이나 블랭크를 두어 학습, 지문중에 빈칸 맞추기 등의 타스크에 그대로 적용
59.dual encoder에서의 사전 학습이란 8개 단어 길이 단위로 다이나 젠에서 코드 압축 결과 시퀀스를 듀얼 인코더에 입력으로 넣고 다음 문장을 
	예측 하고 예측 문자을을 다시 입력의 끝에 추가하여 학습을 반복 하는 것

60.pcode 압축에 따른 정보 손실 복원 방업 - 최종 압축 레이어로 갈수록 히든 사이즈 증가, 최종 듀얼인코더에서 pocde에 해당하는 문장들을
	이전 문장대비 다음 문장 예측 재 학습으로 pcode에 해당하는 long sequence내용을 듀얼 인코어에 회상.
	pcode는 key, 입력은 query, 예측은 응답으로하여 pcode에 대하여 다양한 query, answer를 학습기킴, 위 다음 문장예측도
	쿼리의 한 종류, 이와같이 다양한 query, answer학습에 의하여 듀얼 인코더에서 회상학습함으로써 복원될수있다.

61.양방향 오토 인코딩 사전학습 - 1.지문과 같은 긴 문장을 듀얼 인코더의 pcode(dynagen or generic convolv에의해 압축 생성)로 하여
	key로 만들고 듀얼 인코더의 입력(query)를 예를드면 "회상" 이란 워드로 하여 다시 지문을 타겟으로 오토 리그레션으로 예측 학습한다.
	generic convolv에의해 pcode생성은 사전입력 층까지 역전파 학습된다.
	2.또 지문에 속한 각 문장을 pcode(사전 입력)로 하고 지문에서 다음 문장을 타겟으로 하여 위와 마찬가지 방식으로 예측 학습한다.
	지문 전체이던 문장단위 학습이던 pcode(사전 입력)으로써 입력된 것은 양방향 학습된다.
	3.지문이 없이 하나의 문장을 사전학습한다면 자기지도 학습할 문장을 사전코드(pcode)로하고 입력(쿼리)문장 또한 동일 문장으로 하고
	타겟(응답)문장또한 동일 문장으로 자기지도 학습시킨다. 이때 입력의 첫번째에 go토큰을 삽입하여 리그레션 방식으로 하거나 go토큰을 사용하지않고
	문장의 임의 위치 워드를 블랙크 처리하여 블랭크 없는 동일 문장을 타겟으로 하고 듀얼인코더의 오토 인코딩 방식을 사용하여 학습한다.

	사전학습을 설계하는 것은 그 대상을 무엇으로 하느냐이며 그 선정 기준은 추론과정에서도 동일하게 제시되는 정보로 구성하는 것이다.
	이렇게 사전학습 대상을 선정했으면 동일 정보를 서전입력(pcode), 입력쿼리, 타겟응답으로 구성하고 입력쿼리를 구성할때 오토리그레션 방식으로
	동일 정보로 하거나 현재 듀얼인코더에서 오토인코딩 방식으로 하는데 좀더 구성간 관계를 잘 파악 시키기위해 임의 워드를 블랭크처리 한다.
	이때 정보(문장)가 길지 않을 경우 양방향 학습 필요없다면 pcode는 널로 하고 입력과 타겟만으로 사전학습을 한다. 가급적 사전,사후 학습
	추론 과정에서 동일 한 형식으로 학습시키기위해 리그레션 방식으로 사전학습을 한다.

	4.사전 학습은 독서와 같은 형태로 수행한다. 책 한권을 사전학습 한다면 문단사이즈로 pcode에 적재하고 문장단위로 bygate에 입력파트로서
	"다음 문장은" 이란 입력쿼리명령을 적재하고 타겟파트에 go토큰 + 문장을 적재하고 목표값에 동일문장 + end토큰으로서 학습한다.
	책의 초기에는 이전이 없으므로 책의 시작 워드 한두개를 pcode에 적재하고 첫번째 문장부터 위와 같이 적재하여 학습한다. 학습이 끝난 문장은
	pcode에 옮기고 pcode사이즈가 꽉차면 큐 방식으로 선두 문장을 제거한다. 이과정을 책한권이 끝날때까지 반복한다. 따라서 사전학습은
	단방향 학습이고 양방향 학습은 다운스트림 태스크의 한 종류로서 입력쿼리명령을 "빈칸 맟추기"로 하고 pcode에 빈킨이 포함된 지문을 적재하고 
	정답을 타겟에 적재하여 학습시킨다. 이렇게 하면 문장생성, 챗봇, 번역과 같은 태스크는 사전학습만으로 추론될수있다.

61-2.사후학습(fine tunning) - 61)로 학습후에 61)학습 과정의 파라미터를 고정하든가 미세조정 하든가(최소한 pocde학습 관련 파라미터는 고정)
	하고 듀얼인코더 레이어를 하나 더 추가 하던 추가없이 하던(이럴경우 미세조정 케이스만 해당됨)지 선택하여 구성하고 특정 타스크의 다양한 쿼리와
	응답으로 사후 학습 시킨다. 이때 양방향 학습은 사전학습-자기지도학습 과정에서 pcode의 학습과정에서 이루어지고 번역이나 쳇봇같은 타스크는
	사후학습으로서 양방향이란 개념 자체가 성립되질 않는다. 사후 학습은 리그레션 방식으로 학습시키는데 이의 의미는 다음을 예측할때 바로 직전까지의
	정보를 fully사용한다는 일종의 미분 개념이다.

62.작곡 프로그램 - 음원(pcm) 데이터를 61)의 지문과 같이 사전입력으로 61)방식으로 사전 학습
	음원에서 멜로디 음의 높낮이 길이 정보만을 디치털 신호처리 기술로 추출하여 4분이 1 박자라면 각 한 박자씩 
	0 ~ 1 사이의 데이터로 정규화하여 (16분의 1박자라도 한곡의 데이터로 길지 않을 것임) 쿼리(입력)으로 하고
	이 곡의 음원을 응답(타겟)으로 하여 사후 타스크 학습 시킨다.
	이 후에 주가 곡선을 정규화 하여 쿼리하면 응답으로 이에 적절한 음원 생성.
63.가사 생성 - 가사를 61)의 지문과 같이 사전입력으로 61)방식으로 사전 학습. 
	사후 타스크 학습으로 멤로디를 쿼리로 가사를 응답으로 파인튜닝 학습.
	이 후에 주가 곡선을 정규화 하여 쿼리하면 응답으로 이에 적절한 가사 생성.

64.언어번역 - 1.각 언어별로 문장을 61)으로 사전 학습, 원문 언어의 학습망 + new dual encoder망 + 타겟 언어의 학습망을 연결하여
	원문 망의 입력에 원문 입력으로 넣고 타겟망의 타겟에 번역문을 넣고 ( 단순히 dual encoder interface에 입력과 타겟을 
	넣기만 하면 된다 ) 양측 끝단망의 파라미터를 고정시키고 중간의 듀얼인코더 망만 학습시키면 중간 망에서 교차학습이 될것.
	2.원문을 61)으로 사전학습하고 이 망에 번역문을 입력과 타겟으로 지도학습.

65.자기비판학습 - 학습에 회차 개념으로 두어 1회차 학습후 추론하여 사용후 일정 기간이 경과한 후 변경된 정보들을 반영하기위해 2회차 학습을 
	수행할때 1회차의 가중치의 결과로서 발생한 로스에 반비례한 반영률로서 가중치를 변경한다. 즉, 기존 지식에 반하는 것은 적게
	학습되게 함으로써 보수적으로 학습되게 하여 점진적으로 변경되게 한다.

66.문장완성 블랭크 태그 활용 기법 - 응답문의 단어중에서 숫자, 고유명사와 같이 빈도수가 떨어지는 것을 블랭크 태그로 전처리하여 학습
	학습과정에서 응답 텍스트중에서 [빈도수가 떨어지는 단어들을 블랭크 태깅한 응답문](1)을 입력으로 하고
	[블랭크 태그를 사용하여 질의하는 sql](2)을 타겟으로 하여 학습[SQL-LTCC], 
	[위 (1)과 응답문에서 블랭크 태그에 해당하는 원래값(추론과정이면 디비쿼리하여 획득될 값)들 리스트](3)을 입력으로 하고
	[전처리 전에 블랭크 태깅되지 않은 원 answer문](4)를 타겟으로 하여 학습[NLG-LTCC],
	추론과정에서 답문을 추론한 결과 블랭크 태그가 있으면 
	(1)로서 (2)를 추론하고[SQL-LTCC] (2)를 사용하여 디비에 질의한 결과값으로
	(3)을 만들어서 (4)를 추론하여[NLG-LTCC] 나온 결과 시퀀스로 응답한다.

67. 66의 일반화
	A -> B
	A -> C
	C -> D
	A + D -> B

	C: sql, D: Result set, A: 자연어 query, B: 자연어 answer
	A -> B : 자연어 query to 자연어 answer 연결 데이터, 직접 연결 학습 안함
	A -> C : 자연어 query to sql 연결학습
	C -> D : sql 싫행 결과 Result set
	A + D : 자연어 query + Result set 으로 입력문 구성
	A + D -> B : 자연어 query + Result set to 자연어 answer 연결 학습

	이번 학습에 주어지는 연결 학습 데이터: A -> B, A -> C
	regacy system(이번 예는 DBMS) 또는 선행 학습된 연결망: C -> D
	이번 학습에 자동 구성되는 연결 학습 데이터 : A + D -> B

68.작곡 - 1.습원을 일정 길이 단위, 언어의 예를 든다면 문장 정도 길이, 로 하여 앞 문장을 입력으로 다음 문장을 타겟 학습
	타겟 학습된 문장을 pcode로 이동하여 계속 누적하여 입력으로 하여 다음 문장 타겟 학습, 반복
	2.이런식으로 곡을 음의 높낮이 길이 정보만을 디치털 신호처리 기술로 추출하여 1)방식으로 학습하고
	3.주가 곡선도 1) 방식으로 학습
	4.예측 과정에서 언어의 생성 모델을 적용하여 작곡
	5.주가 곡선이 학습됨으로 인해 기존 노래 곡들에 창조적인 곡이 예측 됨.

69.대량 사전 학습 - 전체 학습 데이터를 하나의 신경망으로 하지 않음, 병렬분산학습 또는 분야별 학습 필요성에 따라
	전체 학습 데이터를 여러개의 신경망으로 나누어 학습, 레벨별로 나누어 최하위 레벨에서부터 전체 데이터를
	분야별 혹은 파트별로 각기 신경망으로 나누어 학습데(시퀀스 단위를 나눈는 것이 아님) 컨볼루션망의 필터와 같은 개념 
	차이점은 각 필터가 데이터를 나누어서(하나의 시퀀스를 나누는 것이 아니라 분야별 혹은 병렬분산 학습 필요성에의해 부분별로 나눔)
	필터별로 각기 다른 데이터를 학습한다는 것, 필터별로 각기 다른 뷰를 갖게됨, 각 레벨별로
	학습, 학숩 완료후 추론, 상위레렙 학습, 학숩후 추론, 그 상위레벨 학습, .. 과 같이 점진적으로 상위로 
	올라가며 학습과 추론이 오버랩된. 학습할때는 데이터에서 각기 다른 부분을 학습후 추론할때는 동일 입력을 추론하여
	필터별고 각기 다른 잠재코드를 출력하고 이 잠재코드를 상위로 출력하고 이를 다시 상위에서 각 부분별록 학습
	상위 레렙로 갈수록 필터의 갯수를 줄여 최상위는 하나의 필터로 하위 동일 데이터 입력에 대한 각기 다른 필터에 의한
	다른 관점 해석을 통합하여 학습 및 추론하여 이결과를 pcode로 하여 오토 리그레션으로 타겟을 연결 학습한다.
	추론 과정에서 주어지는 데이터는 사적학습과 다르게 적은 양일 것이므로 각 레벱 필터들이 동일 데이터를 추론하여 
	동일 데이터에 대한 각기 다른 관점을 추론하고 그 결과 잠재코드를 상위로 출력, 출력이 약한 핕터의 출력은 제외하면
	동적 연결 개념 성립.

70.분산처리 방안 - 플럭스의 매트릭스를 분리한다. 매트릭스 단위로 한대으 기계를 할당하여 매트릭스 머신을 구성한다.
	그래프의 구성과 실행은 코디네이터 호스트에서 하고 매트릭스 연산을 각 기계에서 수행시킨다.
	매트릭스 곱셈에서 primary매트릭스 머신에서 secondary 매트릭스 머신의 매트릭스 데이터를 가져오고
	리턴 매트릭스를 할당하여 행렬 곱셉한후 리턴 매트릭스로 전송한다.

71.경사하강 - 오차를 최소하 하는 죄적화로서 오차함수가 아래로 볼록한 2차 함수라 하면 파라미터(가중치)를 x라하고 오차값을 y라 했을때 
	기울기가 +방향(포물선 우측)은 x를 갑소시키면 y도 감소하고 기울기가 -방향(포물선 왼쪽)은 x를 증가시켜야 y가 감소하게 된다. 
	즉, +방향에서 파라미터(가중치)를 기울기(가중치에 대한 오차값 미분량)만큼 감소(x측에서 좌측으로 이동)시켜야 오차가 감소하게 된다. 
	이를 w = w - dw 로 수행하는 것이 경사하강법이다.
72.정책망 기울기 - 보상을 극대화 하는 최적화로서 보상함수로서 위로 볼록한 2차 함수라 하면 위 경사하강 설명의 오차함수와 반대가 된다. 
	따라서 기울기가 +방향(포물선 좌측)에서 파라미터(가중치)를 기울기(가중치에 대한 보상값 미분량)만큼 증가(x축에서 우측으로 이동)시켜야 
	보상이 증가하게 된다. 이를 경사상승법 w = w + dw 로 수행하는 것이다.
	정책망 기울기의 보상함수는 -log(x)*advantage로서 앞의 - 를 곱한 것은 프레임워크에서 기울기 업데이트가 경사 하강법이 기본으로
	되어있어 이를 경사상승으로 적용하기위해 부호룰 바꾸고자 - 을 곱한 것이고 보상함수는 log(x)*advantage이다. 이의 미분은 
	advantage / x 로서 advantage가 크면 가울기가 크므로 가중치를 더 크게 증가시킨다. 반대로 기울기가 - 방향(포물선 우측)에서는
	패널티(음수 보상값)로 기울기 만큼 가중치를 감소(x축에서 좌측으로 이동)시켜야 보상함수의 값이 극대화 된다. 

73.시계열 강화학습 - 오토리그레션 방식으로 할려면 타겟 파트에 환경에서 주어지는 이미지가 먼저 인코딩되어 액션과 함께 시퀀스가 구성되야 한다.
	이렇게 하지 않고 이미지를 다이나젠에서 인코딩하여 듀얼인코더의 pcode로 주입되야 하는데 이리하면 타겟 파트에 이미지,액션,이미지,액션,,
	과 같이 시퀀스를 구성할 수 없으므로 오토리그레션 방식으로 하지 않고 이미지 시퀀스를 다이나젠에서 압축하여 듀얼 인코더의 pcode로 하고
	액션 시퀀스는 바이게이트에 입력 파트에 적재하여 한게의 타겟을 훈련,에측한다.

74.주식 기반 가상화폐 시스템 - 가상화폐를 최대 회사의 주식 총량 만큼 발행한다. 가상화폐쪽에서는 주식에 기반이 되어 실질 유한 가치가 있으므로 
	실 가치가 되어 가상화폐 가격이 오르고 주식시장쪽에서는 회시의 주식이 가상화폐 시장에서 가치가 있으니 주식 가격이 오른다. 또한 
	회사는 가상화폐를 발행하여 현금을 벌고 그 일부로 유상증자를 하여 주식을 더 발행하여 현금을 또 벌고 또 그 증가된 주식의 일부를 
	가상화폐를 발행하여 또 현금을 벌어들인다. 주식시장과 가상화폐 시장 양측이 지렛대로 가치를 부여하여 양측 자산의 가치가 끝없이 오르고
	회사는 현금을 계속 번다.
	아이템이 있은 회사를 이 거래소에 먼저 상장하여 주식 시장에 상장할때까지 가상퐈폐 발행으로 유동성 자금을 확보하여 주식시장에 상장할
	조건을 충족시키고 주식시장에 상장하여 다시 가상화폐 시장에 가치를 피드한다. 

75.트랜스포머 발전방향 - 입력 시퀀스를 8개 길이로 청크하여 각각을 q,k,v로 동일하게 하여 셀프 어텐션 수행, 이를 임의 횟수만큼 반복수행, 
	이때 이렇게만하면 짝수로만 바인딩되므로 첫번 셀프 어텐션 수행후 이것을 쿼리로하고 청크된 8개 길이 원 입력을 키와 밸류로하여 어텐션 수행하여
	홀수 바인딩도 되게 한다. 다음은 1번 청크를 쿼리로하고 2반 청크를 키와, 밸류로 하여 어켄션수행하여 2개 청크를 1개 청크로 압축하여
	전체 시퀀스 길이를 반으로 줄이고 이 과정을 반복 압축하여 인코딩 과정을 수행, 디코딩 과정은 압축된 코드를 쿼리로 하고 정규분포로부터
	키와 밸류를 생성하여 어텐션 수행후 키와 밸류를 출력으로하여 2배 길이로 확장, 이를 반복하여 원 입력 시퀀스 길이를 복원, 이 결과로
	오토 리그레션 수행 - depricate(실험결과 압축 되지 않음)

76.적합도 시스템 - 1.기존 mempute(심폴릭)를 사용하여 소스와 타겟을 하나의 망에서 각각 1차 패턴 생성후 소스와 타겟을 크로스 조인으로 소스와 조인의
	교집합 패턴 갯수를 합집합 패턴 갯수로 나누어 비율 계산 1에 가까울수록 적합도 높음
	2.mempute(신경망망 을 사용하여 소스와 타겟을 하나의 망에서 각각 오토 인코딩 학습시킨후 크로스 조인으로 소스와 타겟의 압축 코드를 mse오차 계산하여 오자가 가장 적은 
	쌍이 가장 유사도가 높은 소스와 타겟, 이렇게 상위 10%정도 유사도가 높은 쌍을 입력과 타겟으로하여 연결학습 시킨후 위 과정을 오차 범우를
	높혀가며 반복 학습
	3.mempute(신경망망 을 사용하여  소스와 타겟을 하나의 망에서 각각 오토 리그레션 학습시킨후 크로스 조인으로 소스와 타겟을 생성하여 가장 오차가 적은 
	쌍이 가장 유사도가 높은 소스와 타겟, 이렇게 상위 10%정도 유사도가 높은 쌍을 입력과 타겟으로하여 연결학습 시킨후 위 과정을 오차 범우를
	높혀가며 반복 학습

77.심볼릭망 mempute와 신경망 mempute 연계 학습방안 - 압축 패턴 생성 방안
	1)하위부터 상위로 각 레렐별로 심볼릭망으로 패턴추출 -> 입력과 심볼릭 망으로 추출된 패턴을 타겟으로 신영망 지도 학습
	-> 입력에 대하여 학습된 신경망으로 타겟 패턴 추론 -> 추론된(신경망으로부터) 타겟 패턴을 입력으로 심볼릭망에서 패턴 생성 추출하여
	-> 위 교차 하습 과정을 하위에서 상위로 올라가며 반복하여 압출 패턴 생성 -> 2)최종 압축된 패턴을 입력으로 오토 리그레션으로 입력으로 주어진 타겟을 신경망 연결학습하던가
	-> 3)디코딩 과정도 위와 같은 방식으로 하여 하위 패턴을 추론한뒤 4)최종 하위레벨에서 타겟 연결 학습
	-> 5)또한 이렇게 생성된 최종 패턴으로 유사도 측정, 적합도 산출 응용에 적용
	-> 6)첫번째 레렙에서만 임베딩하고 심볼릭망에서 추출된 패턴 아이디를 타겟으로 타겟시퀀스의 각 포지션별로 타겟아이디 갯수만큼 원핫으로 확장하여 크로스엔트로피로 학습시키는데
		상위렐벨로 올릴때는 각 포지션별 원핫확장전의 잠재코드를 올려 상위의 입력이 되게 한다. 이렇게 하여 일반화를 도모한다. => 7)다른 방법은 하위레벨에서 생성된
		패턴아이디를 상위의 입력으로 그대로 올리는 것인데 이렇게 할수있는 근거는 심볼릭 망에서 타겟 패턴을 생성할때 겹치기 금지를 완화하여 다수를 복수로 올려서
		패턴아이디들이 OR 조합으로 타겟 시퀀스가 구성되게 하여 일반화를 할수있기 때문이다.
	-> 8)최하위 레벨에서는 디코더 학습시에 선태된 타겟 패턴아이디 이외 어느 패턴에도 선택되지 못한 입력 심볼을 디코더의 입력에 포함시켜 입력 시퀀스를 타겟으로 학습하게 하고
		이때 토그나이져는 입력과 타겟을 하나로 구성하고 입력에서 선택되지 못한 입력 심볼들을 타겟 디코더의 입력에 포함하여 디코더 학습한다. 
	-> 9)오토 리그레션으로 처리할때는 입력 혹은 타겟 어느측으로 하던 상관없이 어텐션 구성 가능할 정도의 길이로 압축된 레벨에서의 출력시퀀스와 위와 마찬가지로 입력의 최하위
		 레벨에서 선택되지 못한 입력 심볼을 더하여 디코더 어텐션의 키로 하고 타겟 시퀀스를 쿼리와 밸류로 하여 디코더 언덴션을 학습하고 추론시에 오토 리그레션 시킨다.

78.자동 라렙링 방업 - 입력과 타겟을 (입력-타겟은 쌍으로 라벨로서 주어지지 않은 상태) 각각 하나의 심볼릭망과 신경망에서 위 방식으로 압축 패턴생성
	-> 생성된 최종 압축 패턴 아이디를 재구조하여(아이디 리사이징 축소) 단어 임페딩 구성하고 오토 인코딩 방식으로 단어임베딩 학습
	-> 입력군과 타겟군 각각에서 입력문과 타겟문 각각에 대하여 각 군에서 최종 패턴의 유사도를 추출하여 그룹핑 수행
	-> 양축 군(입력과 타겟군)에서 각각 유사 그룹으로 그룹핑된 것을의 멤버 갯수가 각 군간에 동일하고 유클라이안 거리가 비숫한 순서끼리 매칭하여 라벨 생성
	
79.오토 리그레션 방안 - 이미지 처리는 오토리그레션이 필요없고 최종 하위레벨 연결학습도 필요없으므로 77.1)3)으로만 한다.
	입력과 타겟이 나뉘어지고 타겟만 오토리그레션할 경우에는 77.2),9)하면 된다. 
	1)셀프어텐션이나 타겟시퀀스의 처음부터 오토리그레션할 경우는 트랜스포머 어텐션을 사용하지 못하고 77.6)방안을 업워드(선택되지 못한것을 계속 상위로 올림)방식으로 사전학습후에
	목표값 연결학습 단계에서(연결학습망만 학습) 타겟 시퀀스의 처음부터 하나씩 추가하며(선두로부터 일정 갯수는 주어짐-입력문에 해당) 최종 압축 패턴까지
	생성한후 각 리그레션션 단계별로(한단어씩 증가되는) 최종압축패턴 잠재코드(복수일 경우 이어붙임)를 피쳐로하여 원핫 백터를 생성하여 다음 단어를 타겟으로 크로스엔트로피 학습한다.
	학습때 배치처리는 예로 32시퀀스 길이 문장에 잠재코드 사이즈 256, 최종 패턴 갯수 3(최대 사이즈로 일류적으로 정의), 출력원핫백터 10000 이라 하면 32가 배치사이즈가 되어
	(32, 3 * 256) layerdense dot (3 * 256, 10000) => (32, 10000) cross entrophy (10000, 1) => (32, 1)
	목표값 연결 학습때 시퀀스 32를 배치로 구성하는데 있어 예로 시퀀스가 abcd라면 배치의 첫번째 로우부터 차례로 a, ab, abc와 같이 각 배치로우를 구성하고 타겟은 b, c, d
	로하여 연결학습한다.	평가때는 시퀀스를 하나씩 증가하며 오토리그레션으로 수행하면 됨
	
	2)어텐션 오토 리그레션 방안은 1)전문 혹은 질문-답문 쌍을 77.6)방안으로 사전학습후 목표값 연결학습은 1)의 최종압축 또는 중간압축 잠재코드를 키로 하고 답문을 쿼리과 밸류로 
	답랜스포머 디코더를 구성하여 학습한다. 또 사전학습때 전문, 질문, 답문 3개가 있는 경우 전문, 전문+질문, 전문+질문+답문 을 77.6)방식으로 사전학습 시키는 것 다 가능하다.
	
80.문장요약 - 방안 1. 문장을 77.6)방법으로 사전학습후 요약때는 압축 패턴 추출을 학습때보다 더 적게(강도(빈도수)가 더 강한 패턴)을 추출해서 이를 디코더로 복원한다.
	   방안 2. 문장을 77.6)방법으로 사전학습하고 bert방식으로 문장 생성을 학습시킨후 요약때는 압축 패턴 추출을 학습때보다 더 적게(강도(빈도수)가 더 강한 패턴)을 
		추출해서 이의 단말 구성 단어를 bert학습망에 넣고 출력하여 문장으로 변환한다.
	   방안 3. 문서간에 유사도 검새을 하여 같은 유사도 그릅 문서간에 가장 짧은 문서에서 유사도 비교 패턴이 속한 문장만 발췌하여 
		가장 긴 문서의 적당한 레렙까지 압축 패턴을 입력으로 위 추출된 문장을 타겟으로 연결 학습 한다. 연결학습은 layer dense fully connected로 하던 
		79.2)방식으로 한다.

81.장애진단 - 전체 패턴을 학습, 양호 그룹과 불량 그룹이 있을때 각 그룹에서 패턴을 추출한후 불랼그룹패턴 - 양호그룹패턴을 하여 주로 불량그룹에만 있는 패턴들을
	추출한후 이들의 강도 평균을 내고 강도 아웃터를 피치강도로 하여 이 이상은 패턴추론 과정에서 제거하고 그 이하 패턴들만 추출하여 이를 입력으로 하고
	불량, 양호 태그로 크로스엔트로피 신경망 목표값 연결학습

82.시퀀스 오더 무시 방법은 시퀀스를 아이디 정수 값 순으로 잉률적으로 정렬 전처리를 한다.
83.todo - 레렙 별로 학습 및 피쳐 추출 api 추가
84.art system병령 처리 방안 - 학습 데이터를 병렬 처리 갯수로 등분한 배치를 각각 cleo를 할당하여 병렬로 수행하고 퍼셉 디비는 공유하여 컨브 학습의 경우
	레벨 단위로 각 병렬 cleo실행의 수행 시점만 동시에 시작하게 동기화 한다.

python tip
seq = '123456789abc'
length=3
a = map(''.join, zip(*[iter(seq)]*length))
print(*a) #123 456 789 abc
zip(*[iter(seq)]*length)는 zip(*([iter(seq)]*length)) 이고
[iter(seq)]*length]는 [iter(seq) iter(seq) iter(seq)]가 되고 같은 iter객체 포인터를 3번 복사하여 배열 생성한 것이 됨.
*는 evaluate의미이고 *[iter(seq) iter(seq) iter(seq)]는 iter(seq) iter(seq) iter(seq)
zip(iter(seq) iter(seq) iter(seq))는 같은 iter객체의 next 오퍼레이젼을 세번씨 호출하여 zip연산을 하게되므로 포인터는 계속증가하여
('1', '2', '3') ('4', '5', '6') ('7', '8', '9') ('a', 'b', 'c') 가 되고
map은 위 각 4개 그룹 각각을 iteration하여 그 원소들을 조인 연산수행하여 결과는 123 456 789 abc 가 된다.
equal) a = [  ''.join, for x in zip(*[list(seq[z::length]) for z in range(length)])  ]
equal) a = [seq[i:i+length] for i in range(0, len(seq), length)]
	


DFLT_INTERVCUT_ELAST_RT,  DFLT_INTERVCUT_INTENSE_RT 값 조정
ca_ith를 partial conv length한도에서 입력 시퀀스상 첫번째로부터 남을 길이 설정
listConvRegress(1672)에서 convStrenFirst를 자르지 않고 강동 우선으로 키구성 검토 
evaluateFeature에서 추출 갯수가 일정 이하이면 해당 장면 학습 더 하지않게 mpOutFeat쓰지 않는 것 검토

